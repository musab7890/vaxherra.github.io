{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bacterial name generator with RNNs\n",
    "\n",
    "In this post I’ll cover how to build a Recurrent Neural Network (RNN) from scratch using `python3.6` with basic numerical libraries like `numpy`. Then I'll train this neural network using bacterial names from a popular microbialdatabase and *'sample'* from this trained network to come up with novel names for bacteria, that have not been observed in our training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Getting and formatting the data\n",
    "\n",
    "First part is obtaining the dataset. I am going to download a fairly popular `SILVA` database. \n",
    "\n",
    "> *A comprehensive on-line resource for quality checked and aligned ribosomal RNA sequence data. \n",
    "> SILVA provides comprehensive, quality checked and regularly updated datasets of aligned small (16S/18S, SSU) and large subunit (23S/28S, LSU) ribosomal RNA (rRNA) sequences for all three domains of life (Bacteria, Archaea and Eukarya). *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am only going to download taxonomy annotations from [132 release of SILVA](https://www.arb-silva.de/no_cache/download/archive/release_132/Exports/taxonomy/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-23 14:51:51--  https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/SILVA_132_LSUParc_tax_silva.fasta.gz\n",
      "Resolving www.arb-silva.de (www.arb-silva.de)... 134.102.40.6\n",
      "Connecting to www.arb-silva.de (www.arb-silva.de)|134.102.40.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 272844631 (260M) [application/gzip]\n",
      "Saving to: ‘SILVA_132_LSUParc_tax_silva.fasta.gz’\n",
      "\n",
      "SILVA_132_LSUParc_t 100%[===================>] 260.20M  4.84MB/s    in 61s     \n",
      "\n",
      "2018-09-23 14:52:53 (4.29 MB/s) - ‘SILVA_132_LSUParc_tax_silva.fasta.gz’ saved [272844631/272844631]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/SILVA_132_LSUParc_tax_silva.fasta.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Formatting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">AY187551.1.496 Bacteria;Proteobacteria;Alphaproteobacteria;Rhizobiales;Xanthobacteraceae;Bradyrhizobium;Bradyrhizobium sp. Ih3-2\n",
      "CUACGCUGCGAUAAGCCGUGGGGAGCUGCGAAGAAGCUUUGAUCCACGGAUUUCCGAAUGGGGAAACCCACCUUCGAUAG\n",
      "CCGGAACUCCAAGACCUUUGUCGAAAGACAUCGGUGUGGGGUUCAACCAGACAAUGGAAGAAGCCAGGCCUUUAGAUUUC\n",
      "GAUCGAAGAGGUUUUGGAUUUCCGGUUAUCAAGUGAAGGUAUGAGACUUCUGAAUAAAAUAGGAGGUUUCAAGCAAACCC\n",
      "AGGGAACUGAAACAUCUAAGUACCUGGAGGAAAGGACAUCAACAGAGACUCCGUUAGUAGUGGCGAGCGAACGCGGACCA\n",
      "GGCCAGUGAUACAUCAAAGACAAUCGGAACCAGUCAGGAAAGCUGGGCCUCAGAGGGUGAUAGCCCCGUACGAGUAAUGC\n",
      "GAUGAUGUAUCCACGAGUAAGGCGGGACACGUGAAAUCCUGUCUGAACACGGGGGGACCACCCUCCAAGCCUAAGUACUC\n",
      "CUCAGCGACCGAUAGC\n",
      ">AY187559.1.472 Bacteria;Proteobacteria;Alphaproteobacteria;Rhizobiales;Xanthobacteraceae;Bradyrhizobium;Bradyrhizobium sp. Vp10-1\n",
      "CUACGCUGCGAUAAGCCGUGGGGAGCUGCGAAGAAGCUUUGAUCCGCGGAUUUCCGAAUGGGGAAACCCACCUUCGAUAG\n",
      "CCGGAACUCCAAAACCUUGUGGUUUUGGGGUUUGACCAGAAAUGAUCGGAACCAUGACCUCUUGAGGUUUUGGAUUUCCG\n",
      "GUUAUCAAGAGAAGGUAUGAGACCUCCGAAUAAAAUAGGAGGUUUUAAGCAAACCCAGGGAACUGAAACAUCUAAGUACC\n",
      "UGGAGGAAAGGACAUCAAUCGAGACUCCGUUAGUAGUGGCGAGCGAACGCGGACCAGGCCAGUGAUACAUCAAAGACAAU\n",
      "CGGAACCUGUCAGGAAAGCAGGGCCUCAGAGGGUGAUAGCCCCGUACGAGUAAUGCGAUGAUGUAUCCACGAGUAAGGCG\n",
      "GGACACGUGUAAUCCUGUCUGAACACGGGGGGACCACCCUCCAAGCCUAAGUACUCCUCAGCGACCGAUAGC\n",
      ">AY190663.649.1113 Bacteria;Proteobacteria;Gammaproteobacteria;Pseudomonadales;Pseudomonadaceae;Pseudomonas;Sorangium cellulosum\n",
      "GGUCAAGUGAAGAAGCGCAUACGGUGGAUGCCUUGGCAGUCAGAGGCGAUGAAAGACGUGGUAGCCUGCGAAAAGCUUCG\n",
      "GGGAGUCGGCAAACAGACUUUGAUCCGGAGAUGUCUGAAUGGGGGAACCCACCUAACAUAAGUUAGGUAUCUUAAGCUGA\n",
      "AUACAUAGGCUUAAGAAGCGAACCAGGGGAACUGAAACAUCUAAGUACCCUGAGGAAAAGAAAUCAACCGAGAUUCCCUU\n",
      "AGUAGUGGCGAGCGAACGGGGACCAGCCCUUAAGCUGUAUUGAUGUUAGCGGAACGCUCUGGAAAGUGCGGCCAUAGUGG\n",
      "\n",
      "gzip: stdout: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!zcat SILVA_132_LSUParc_tax_silva.fasta.gz | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a standard `.FASTA` file that contains nucleotide sequences (that we'll discard) and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data itself needs to be reformatted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat SILVA_132_LSUParc_tax_silva.fasta.gz | grep \">\" | cut -f2 | cut -f6 -d ';' | cut -f1 -d ' '  | sed 's/[^a-zA-Z ]/ /g' | awk '{print $1}'  | sort | uniq | sed  '/^$/d' | sed -r '/^.{,3}$/d' > genera.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our formatting is a set of simple pipes, that I'll shortly explain:\n",
    "\n",
    "- `grep \">\"` catches all headers from a $.fasta$ file. We are essentially throwing away nucleotide sequences,\n",
    "- `cut -f2` cuts the second collumn with taxonomy annotations\n",
    "- `cut -f6 -d ';'` cuts sixth columns containing genera names that are separated by `;`\n",
    "- `cut -f1 -d ' '` cuts first column from genera names that sometimes have two words. As our model will only work on a single word for a bacteria, this is sufficient,\n",
    "- `sed 's/[^a-zA-Z0 ]/ /g` - replaces all non letters with a blank space,\n",
    "- `awk '{print \\$1}'`  - print the first column of the data\n",
    "- `sort` - sorts results alphabetically, so duplicates are put next to each other\n",
    "- `uniq` removes all duplicates\n",
    "- `sed  '/^$/d'` removes empty rows\n",
    "- ` sed -r '/^.{,3}$/d'` removes rows with words shorten than 3 characters (letters like A, XY, etc...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abelmoschus\n",
      "Abies\n",
      "Abiotrophia\n",
      "Abolboda\n",
      "Abortiporus\n",
      "Abrahamia\n",
      "Abroma\n",
      "Abrus\n",
      "Absidia\n",
      "Abuta\n"
     ]
    }
   ],
   "source": [
    "!head -10 genera.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zosterograptus\n",
      "Zoysia\n",
      "Zunongwangia\n",
      "Zygnematales\n",
      "Zygoascus\n",
      "Zygodon\n",
      "Zygophyllum\n",
      "Zygostates\n",
      "Zymobacter\n",
      "Zymomonas\n"
     ]
    }
   ],
   "source": [
    "!tail -10 genera.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later I'll replace uppercase characters with lowercase characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7903\n"
     ]
    }
   ],
   "source": [
    "!cat genera.txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all our training dataset has around 79 hundred examples. This should be sufficient for training our RNN network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the data\n",
    "\n",
    "We need to load the data. We'll convert all characters to lowercase as model doesn't need to learn something that is not useful for our task.\n",
    "\n",
    "The majority of NLP (Natural Language Processing) systems work on some kind of ecodings for processed words. Here we generate `chars`, an 'alphabeth' of characters that is present in our dataset. The function ` set` creates a \"set\" of unique elements, and `list` converts them for conenivence.\n",
    "\n",
    "We create `char_to_ix` that converts given character to an index, and the reverse operation (from index to a character) is carried out by `ix_to_char` dictionary. We are ready to build our model (from the scratch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 89903 total characters and 27 unique characters in your data.\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "data = open('genera.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))\n",
    "\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) } \n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) } \n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect here that we have 26 latin characters + 1 new line character that is going to indicate in our model that the bacterial name has finished being generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# II. Building an RNN architecture in Python and Numpy\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement our architecture in Python with only the use of numpy. We have to define all the functions ourselves. Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Softmax](https://en.wikipedia.org/wiki/Softmax_function) is a normalized exponential function. Its input is a $K$ dimensional vector of real values that get normalized to be in the range $\\in (0,1)$, where sum of this vectors adds up to $1$:\n",
    "\n",
    "$$ softmax(z) = \\frac{e^{z_j}}{\\Sigma^K_{k=1}e^{z_k}} $$ where: $j=1,2,...,K$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Sigmoid:\n",
    "\n",
    "A simple function [defined as:](https://en.wikipedia.org/wiki/Sigmoid_function)\n",
    "\n",
    "$$ \\sigma(x)= \\frac{e^x}{1+e^x} = \\frac{1}{1+e^{-x}} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Other helper functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will used later for RNN training. I borrowed these from [Andrew Ng's Sequence Models course](https://www.coursera.org/learn/nlp-sequence-models/home/welcome). The idea is to \"smooth\" a little bit the cumulative loss function, by using loss function for current computation `curr_loss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(loss, cur_loss):\n",
    "    return loss * 0.999 + cur_loss * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_loss(vocab_size, seq_length):\n",
    "    return -np.log(1.0/vocab_size)*seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each character is converted to a one-hot coded vector (later). We need a function that takes a series of numbers (indexed) that correspond to created word, and a dictionary mapping index to a character (`ix_to_char`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_ix, ix_to_char):\n",
    "    '''\n",
    "    Takes a list of indexes in `sample_ix` and a conversion array `ix_to_char`\n",
    "    '''\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    print ('%s' % (txt, ), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "beab"
     ]
    }
   ],
   "source": [
    "print_sample([0,2,5,1,2],ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bulding blocks of RNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Parameters initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed with building NN architecture we need to initialize our parameters $W_{ax}, W_{aa}, W_{ya}, b, b_y$ to some (non-zero) values. We are implementing a vectorized implementation:\n",
    "\n",
    "- input vector data `X` contains `m` examples, each of size `n_x`; its size is (`n_x`,`m`)\n",
    "- previous hidden state $a^{<t-1>}$ is of size (`n_a`,`m`)\n",
    "- bias of the hidden state `b_a` is of shape (`n_a`,`1`)\n",
    "- bias of the output `b_y` is of shape (`n_y`,`1`)\n",
    "\n",
    "These size imply sizes of our matrices:\n",
    "- size(`Wax`)=`(n_a, n_x)`\n",
    "- size(`Waa`)=`(n_a, n_a)`\n",
    "- size(`Wya`)=`(n_y, n_a)`\n",
    "\n",
    "We use numpy `randn` function and multiply resulting random number by `0.01` to obtain small, random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
    "    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
    "    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
    "    b = np.zeros((n_a, 1)) # hidden bias\n",
    "    by = np.zeros((n_y, 1)) # output bias\n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Building RNN\n",
    "\n",
    "#### 2.2.1. Forward pass through a cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network is comprised of repeated blocks for each time step $x^{<t>}$ as shown below on the diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=RNNcell.png width=900 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our building block takes as input previous hidden state $a^{<t-1>}$ and current (already one-hot encoded) input $x^{<t>}$ and computes next hidden state $a^{<t>}$ and `softmax` prediction $\\hat{y}^{<t>}$. This in implemented in `rnn_step_forward` python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    \n",
    "    # Retreive parameters from a dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    # Compute next hidden state\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # hidden state\n",
    "    \n",
    "    #Compute softmax probability\n",
    "    p_t = softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars # probabilities for next chars \n",
    "    \n",
    "    #Return next hidden state and probability\n",
    "    return a_next, p_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Forward pass through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass through the network is a repetition of single blocks implemented in section 2.2.1. Each cell takes an input the hidden state of previous cell $a^{<t-1>}$ and current time-step $x^{<t>}$ of input sequence $X$.\n",
    "\n",
    "<img src=RNNarch.png width=900 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass needs to compute vectors of $\\hat{Y}$ predictions and hidden states `'a'` and `'caches'` for computing gradient in backpropagation step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function is a cross entropy. For a given time-step $t$ it is defined as:\n",
    "$$ L_t(y_t,\\hat{y}_t) = - y_tlog(\\hat{y}_t) $$\n",
    "\n",
    "for an entire sequence cost is just a sum of these values\n",
    "\n",
    "$$ L(y,\\hat{y}) = - \\Sigma_t y_tlog(\\hat{y}_t) $$\n",
    "\n",
    "so durng each forward pass we update the cost function  $ L(y,\\hat{y})$ by loss on single time-step $ L_t(y_t,\\hat{y}_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(X, Y, a0, parameters, vocab_size = vocab_size):\n",
    "    \n",
    "    # Initialize x, a and y_hat as empty dictionaries\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    \n",
    "    a[-1] = np.copy(a0) # First hidden state (initial), in the figure above as a0, is here represented by index -1.\n",
    "    \n",
    "    # initialize your loss to 0\n",
    "    loss = 0\n",
    "    \n",
    "    # We loop over our input sequence X (Tx, n_x,m), through time steps t in Tx\n",
    "    for t in range(len(X)):\n",
    "        \n",
    "        # Set x[t] to be the one-hot vector representation of the t'th character in X.\n",
    "        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n",
    "        x[t] = np.zeros((vocab_size,1)) \n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        \n",
    "        # Run one step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
    "        \n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "        loss -= np.log(y_hat[t][Y[t],0])\n",
    "        \n",
    "    cache = (y_hat, a, x)\n",
    "        \n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Backpropagation pass through a cell \n",
    "\n",
    "Backpropagation in an RNN is called somewhat fancy: backpropagation through time. As somebody that had some exposure to physics and quantum mechanics I twitch with a convulsion, but hey, the whole field is a giant hype, and the more fancy the terms, the more people's attention (and money) perhaps it will pull. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is the most complicated problem in this post. In my neighboring post where I implement everything in Keras this would not be the case, as existing frameworks \"outsource\" the need to compute derivatives for the framework itself. \n",
    "\n",
    "However it is better to follow a simple rule \"unless you build it yourself, you don't really understand it\". Presumably it is attributed to Richard Feynman, but who really knows.\n",
    "\n",
    "I'll reproduce our RNN cell, so it will become clear how computations are derived:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=RNNcell.png width=900 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for a given cell we are given a set of equations. I am going to place all of the here so it will serve as a reference:\n",
    "\n",
    "$\\hat{y}^{<t>}$ - this is our (model's) prediction for a given time-step $t$.\n",
    "\n",
    "1. $L_t = - y^{<t>} log(\\hat{y}^{<t>})$\n",
    "2. $\\hat{y}^{<t>} = softmax ( W_{ya} a^{<t>} + b_y ) = softmax(z)$ (I add a helper variable $z$ for clarity of derivations)\n",
    "3. $a^{<t>} = tanh  (W_{ax} x^{<t>} + W_{aa}a^{<t-1>} + b_a)$\n",
    "\n",
    "where:\n",
    "\n",
    "$$softmax(z_j) =  \\frac{e^z_j}{\\Sigma^K_k e^z_k}$$ where $j=1,2,...k,k+1,...,K$$\n",
    "\n",
    "$$tanh(z) = \\frac{sinh(z)}{cosh(z)} =  \\frac{e^z - e^{-z}}{e^z + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **a) Softmax and tanh derivatives **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've [looked up](http://www.analyzemath.com/calculus/Differentiation/hyperbolic.html) the derivative of $tanh(z)$:\n",
    "\n",
    "$$\\frac{\\partial tanh(z)}{\\partial z} = sech^2(z)$$\n",
    "\n",
    "Then one can use one of [hyperbolic identities](http://math2.org/math/trig/hyperbolics.htm):\n",
    "\n",
    "$$ tanh^2(z) + sech^2(z) = 1 \\Rightarrow  sech^2(z) = 1 - tanh^2(z) $$\n",
    "\n",
    "\n",
    "So:\n",
    "$$\\frac{\\partial tanh(z)}{\\partial z}  = 1 - tanh^2(z) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's proceed to a $softmax$ derivative:\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial softmax(z_j))}{\\partial z_i} = \\frac{1}{\\partial z_i} (\\frac{e^z_j}{\\Sigma^K_k e^z_k} )  $$\n",
    "where $j=1,2,...k,k+1,...,K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to consider two situations. One, where indices $i=j$, the second where $i\\neq j$. \n",
    "\n",
    "- if $i=j$, then the derivate of softmax will *look like* (just to present you):\n",
    "\n",
    "$$ \\frac{\\partial softmax(z_j))}{\\partial z_i} =  \\frac{1}{\\partial z_i} (\\frac{e^z_j}{e^z_j + \\Sigma_{k, k\\neq j}^K e^z_k })   $$ \n",
    "\n",
    "and $\\Sigma_{k, k\\neq j}^K e^z_k$ is essentially a *constant* part in calculating derivation, and for clarity I'll use $const.$ to represent it:\n",
    "\n",
    "$$ \\Sigma_{k, k\\neq j}^K e^z_k \\equiv const.  $$ \n",
    "\n",
    "so there it goes:\n",
    "\n",
    "$$  \\frac{\\partial softmax(z_i))}{\\partial z_i} =  \\frac{1}{\\partial z_i} (\\frac{e^z_j}{e^z_j + const.})    $$\n",
    "\n",
    "I'll also use a basic rule of calculus - [quotient rule](http://tutorial.math.lamar.edu/Classes/CalcI/ProductQuotientRule.aspx), to recall: \n",
    "\n",
    "$$ \\frac{1}{\\partial x} (\\frac{f(x)}{g(x)}) = \\frac{ \\frac{\\partial f(x)}{\\partial x} g(x) - f(x) \\frac{\\partial g(x)}{\\partial x}   }{g(x)^2} $$\n",
    "\n",
    "\n",
    "Finally, we'll have: \n",
    "\n",
    "$$ \\frac{\\partial softmax(z_j))}{\\partial z_i} =  \\frac{ \\frac{\\partial e^z_j}{\\partial z_i} (e^z_j + const.) - e^z_j \\frac{\\partial e^z_j + const.}{\\partial z_i} }{ (e^z_j + const.)^2} = \\frac{ e^z_j (e^z_j + const.) - e^z_j \\cdot e^z_j}{ (e^z_j + const.)^2}  $$\n",
    "\n",
    "We can use our $softmax$ definition to come up with:\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial softmax(z_j))}{\\partial z_i} =  softmax(z_j) - softmax^2(z_j) $$ \n",
    "**if $i=j$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if $j \\neq i $\n",
    "\n",
    "$$ \\frac{\\partial softmax(z_j))}{\\partial z_i} =  \\frac{1}{\\partial z_i} (\\frac{e^z_j}{e^z_i + \\Sigma_{k, k\\neq i}^K e^z_k })   $$ \n",
    "\n",
    "Here we can treat $e^z_j$ and $ \\Sigma_{k, k\\neq i}^K e^z_k$ as constant. We compute derivatives over $i$ index, over $e^z_i$. So this could be viewed as calculating a simple derivative:\n",
    "\n",
    "$$ \\frac{\\partial }{\\partial x} (\\frac{contst_1}{e^x + constant_2})  = const_1 \\cdot  \\frac{\\partial }{\\partial x} (\\frac{1}{e^x + constant_2}) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again use a quotient rule, but it is going to be much simpler:\n",
    "\n",
    "\n",
    "$$ ... = const_1 \\cdot \\frac{ 0 \\cdot (e^x + constant_2) - 1 \\cdot e^x }{(e^x + constant_2 )^2}  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so:\n",
    "$$  \\frac{\\partial softmax(z_j))}{\\partial z_i} = e^z_j \\cdot \\frac{1}{\\partial z_i} (\\frac{1}{e^z_i + \\Sigma_{k, k\\neq i}^K e^z_k})  =  ...$$\n",
    "\n",
    "\n",
    "$$ ... =  e^z_j \\cdot \\frac{ \\frac{1}{\\partial z_i}(1) \\cdot (e^z_i + \\Sigma_{k, k\\neq i}^K e^z_k) - 1 \\frac{1}{\\partial z_i}(e^z_i + \\Sigma_{k, k\\neq i}^K e^z_k)   }{(e^z_i + \\Sigma_{k, k\\neq i}^K e^z_k)^2 } = ... $$\n",
    "\n",
    "\n",
    "$$ ... = e^z_j \\cdot \\frac{ 0  - e^z_i }{(e^z_i + \\Sigma_{k, k\\neq i}^K e^z_k)^2} = \\frac{ - e^z_j \\cdot  e^z_i }{(e^z_i + \\Sigma_{k, k\\neq i}^K e^z_k)^2}  = \\frac{ - e^z_j \\cdot  e^z_i }{( \\Sigma_{k}^K e^z_k)^2} = - softmax(z_i) \\cdot softmax(z_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, just to summarize our results and have a *clear* reference:\n",
    "\n",
    "\n",
    "- if $i=j$\n",
    "$$ \\frac{\\partial softmax(z_j))}{\\partial z_i} =  softmax(z_j) - softmax^2(z_j),  \\text{     }\\text{     }\\text{          if } i=j $$ \n",
    "- if $i \\neq j$\n",
    "$$\\frac{\\partial softmax(z_j))}{\\partial z_i} = - softmax(z_i) \\cdot softmax(z_j),  \\text{     }\\text{     }\\text{          if } i\\neq j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **b) Calculating:** $\\frac{\\partial L_y}{\\partial W_{ya}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to compute derivatives of the cost function $L$ for a single cell with relation to weights. Let's compute the first derivative $ \\frac{\\partial L_t}{W_{ya}}$.\n",
    "\n",
    "Let's recall the functions that we'd be derivating over:\n",
    "1. $L_t = - y^{<t>} log(\\hat{y}^{<t>})$\n",
    "2. $\\hat{y}^{<t>} = softmax (z )$\n",
    "3. $z =  W_{ya} a^{<t>} + b_y$\n",
    "\n",
    "Following a simple chain rule we can write:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L_t}{\\partial W_{ya}} = \\frac{\\partial L_t}{\\partial \\hat{y}^{<t>}} \\cdot \\frac{\\partial \\hat{y}^{<t>}}{\\partial z} \\cdot  \\frac{\\partial z}{\\partial W_{ya}} $$ \n",
    "\n",
    "We can already see that: $\\frac{\\partial z}{\\partial W_{ya}} = a^{<t>}$. We also already computed derivative of $softmax$. Let's compute: $\\frac{\\partial L_t}{\\partial \\hat{y}^{<t>}}$\n",
    "\n",
    "It is nice to remember that $ \\frac{1}{\\partial x} (log(x)) = \\frac{1}{x} $. With that we are properly equipped to solve:\n",
    "\n",
    "$$\\frac{\\partial L_t}{\\partial \\hat{y}^{<t>}} = \\frac{\\partial}{\\partial \\hat{y}^{<t>}} (- y^{<t>} log(\\hat{y}^{<t>}))  =  \\frac{- y^{<t>}}{\\hat{y}^{<t>}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put everything here together:\n",
    "\n",
    "$$ \\frac{\\partial L_t}{\\partial W_{ya}} = \\frac{\\partial L_t}{\\partial \\hat{y}^{<t>}} \\cdot \\frac{\\partial \\hat{y}^{<t>}}{\\partial z} \\cdot  \\frac{\\partial z}{\\partial W_{ya}}  = \\frac{- y^{<t>}}{\\hat{y}^{<t>}} \\cdot  \\Big( ( softmax(z_i) - softmax^2(z_j)  ) + (- softmax(z_i)\\cdot softmax(z_j)) \\Big) \\cdot a^{<t>} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first let's make sure we have proper notation. What is $\\hat{y}^{<t>}$? It is our model's prediction. And this prediction is made with $softmax$ of course. Hence, we have to unify our notation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L_t}{\\partial z} = \\frac{- y^{<t_i>}}{\\hat{y}^{<t_i>}} \\cdot (\\hat{y}^{<t_i>} - \\hat{y}^{2<t_i>})  + \\Sigma^K_{k,k\\neq i} (\\frac{- y^{<t_k>}}{\\hat{y}^{<t_k>}}) \\cdot (- \\hat{y}^{<t_k>} \\cdot \\hat{y}^{<t_i>}) ...  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is simplified as\n",
    "\n",
    "$$ ... =  - y^{<t_i>} + y^{<t_i>}\\hat{y}^{<t_l>} + \\Sigma^K_{k,k\\neq i} y^{<t_k>}\\hat{y}^{<t_i>}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the sum $\\Sigma^K_{k,k\\neq i}$ that goes over every element except $k=l$, but there is also one term $y^{<t_i>}\\hat{y}^{<t_l>}$ that adds this \"except\" term. We can thus simplify:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L_t}{\\partial z} = -y^{<t_l>} + \\hat{y}^{<t_l>} \\Sigma_k^K y^{<t_k>} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, just to recap. $\\hat{y}$ are predictions, they take values between 0 and 1, and sum up to one. But, $y$ (without this funny hat) are *'ground truth'* labels. Essentially $y$ is one-hot coded vector that has one $'1'$ and the rest $'0'$. It also sums to one obviously. Thus, we can further simplify:\n",
    "\n",
    "$$ \\frac{\\partial L_t}{\\partial z} = -y^{<t_l>} + \\hat{y}^{<t_l>}  = \\hat{y}^{<t_l>} -y^{<t_l>}  $$\n",
    "\n",
    "So essentially prediction $\\hat{y}$ minus the true label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L_t}{\\partial W_{ya}}  =  \\frac{\\partial L_t}{\\partial z} \\frac{\\partial z}{\\partial W_{ya}} =  (\\hat{y}^{<t_l>} -y^{<t_l>} ) \\cdot a^{<t>}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus in our code in we write \n",
    "\n",
    "```python\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add this gradients for each time-step $t$. As our total cost function is a sum of costs for each time step:\n",
    "\n",
    "$$ L = \\Sigma^T_{i=0} L_i $$\n",
    "\n",
    "so:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W_{ya}} = \\Sigma^T_{i=0}  \\frac{\\partial L_i}{\\partial W_{ya}}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **c) Calculating:** $\\frac{\\partial L_y}{\\partial b_y}$ \n",
    "\n",
    "This will be simple as we've already computed most of derivatives:\n",
    "\n",
    "$$ \\frac{\\partial L_t}{\\partial b_{y}} = \\frac{\\partial L_t}{\\partial \\hat{y}^{<t>}} \\cdot \\frac{\\partial \\hat{y}^{<t>}}{\\partial z} \\cdot  \\frac{\\partial z}{\\partial b_y} $$ \n",
    "\n",
    "We essentialy don't have the last component which is equal to 1, as $z =  W_{ya} a^{<t>} + b_y$, so $\\frac{\\partial z}{\\partial b_y} = 1$\n",
    "\n",
    "So we're left with \n",
    "$$ \\frac{\\partial L_t}{\\partial b_{y}} = \\frac{\\partial L_t}{\\partial \\hat{y}^{<t>}} \\cdot \\frac{\\partial \\hat{y}^{<t>}}{\\partial z} \\cdot 1 = \\frac{\\partial L_t}{\\partial z} = \\hat{y}^{<t_l>} -y^{<t_l>} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence in our code we'll write\n",
    "```python\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "    #...\n",
    "    gradients['dby'] += dy\n",
    "    #...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **d) Calculating:** $W_{aa}$ (time) gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap our RNN cell:\n",
    "<img src=RNNcell.png width=900 >\n",
    "\n",
    "And our equations (I added a bit of helper notations, $p^{<t>}$ and $z^{<t>}$, in order to split them into several equations for the clarity):\n",
    "1. $L_t = - y^{<t>} log(\\hat{y}^{<t>})$\n",
    "2. $\\hat{y}^{<t>}  = softmax(z)$ \n",
    "3. $z^{<t>}=  W_{ya} a^{<t>} + b_y$\n",
    "4. $a^{<t>} = tanh(p)$\n",
    "5. $p^{<t>} = W_{ax} x^{<t>} + W_{aa}a^{<t-1>} + b_a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to compute now is $\\frac{\\partial a^{<t>}}{\\partial W_{aa}}$. \n",
    "\n",
    "Lets use (again) a chain rule for our equations:\n",
    "\n",
    "$$ \\frac{\\partial a^{<t>}}{\\partial W_{aa}} = \\frac{ \\partial a^{<t>} }{\\partial p}   \\frac{\\partial p}{\\partial W_{aa}} $$ \n",
    "\n",
    "We already know $\\frac{ \\partial a^{<t>} }{\\partial p} $ is just a derivative of `tanh` function we've computed before: \n",
    "$$\\frac{\\partial tanh(z)}{\\partial z}  = 1 - tanh^2(z) $$\n",
    "\n",
    "And $\\frac{\\partial p}{\\partial W_{aa}} $ is a simple derivative:\n",
    "$$ \\frac{\\partial p}{\\partial W_{aa}}  = a^{<t-1>} $$\n",
    "\n",
    "Combining these we get:\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial a^{<t>}}{\\partial W_{aa}} = (1 - tanh^2(p^{<t>})  \\cdot a^{<t-1>} = (1-a^{2<t>})a^{<t-1>} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we write in our code:\n",
    "\n",
    "```python\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    #...\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    #...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily compute other gradient $\\frac{\\partial a^{<t>}}{\\partial b_a}$ from the go:\n",
    "\n",
    "$$\\frac{\\partial a^{<t>}}{\\partial b_a} = \\frac{\\partial a^{<t>}}{\\partial p} \\cdot \\frac{\\partial p}{\\partial b_a} = (1-a^{2<t>})$$\n",
    "\n",
    "Thus in our code we write:\n",
    "\n",
    "```python\n",
    "\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    #....\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    #...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **e) Calculating:** $W_{ax}$ (input) gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of calculations are done. This doesn't need much explanation:\n",
    "\n",
    "$$\\frac{\\partial a^{<t>}}{\\partial W{ax}} = \\frac{\\partial a^{<t>}}{\\partial p} \\cdot \\frac{\\partial p}{\\partial W_{ax}} = (1-a^{2<t>}) \\cdot x^{<t>}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, our code has:\n",
    "\n",
    "```python\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    #...\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    #...\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    #...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to compute gradients with of our loss function for a single example $L_t$ but with respect to $a^{<t>}$.\n",
    "\n",
    "Again, I am going to recall our equations for clarity:\n",
    "1. $L_t = - y^{<t>} log(\\hat{y}^{<t>})$\n",
    "2. $\\hat{y}^{<t>}  = softmax(z^{<t>})$ \n",
    "3. $z^{<t>}=  W_{ya} a^{<t>} + b_y$\n",
    "4. $a^{<t>} = tanh(p)$\n",
    "5. $p^{<t>} = W_{ax} x^{<t>} + W_{aa}a^{<t-1>} + b_a$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L_t}{\\partial a_{<t>} }  = \\frac{\\partial L_t}{\\partial \\hat{y}^{<t>}} \\cdot  \\frac{\\partial \\hat{y}^{<t>}}{\\partial z^{<t>} } \\cdot  \\frac{\\partial z^{<t>} }{\\partial a_{<t>}} $$\n",
    "\n",
    "The first two derivatives we have already solved.\n",
    "\n",
    "$$ \\frac{\\partial L_t}{\\partial z}  = \\hat{y}^{<t_l>} -y^{<t_l>} = `dy` \\text{ in our code } $$\n",
    "\n",
    "$$  \\frac{\\partial z^{<t>} }{\\partial a^{<t>}}  = W_{ya}$$ \n",
    "\n",
    "So we get:\n",
    "$$ \\frac{\\partial L_t}{\\partial W_{ya} } = (\\hat{y}^{<t_l>} -y^{<t_l>}) W_{ya} $$ \n",
    "\n",
    "Which we implement in code as:\n",
    "```python\n",
    " da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, the final (!) derivative: \n",
    "\n",
    "$$ \\frac{\\partial a^{<t>}}{\\partial a^{<t-1>}}  = \\frac{\\partial a^{<t>} }{\\partial p } \\cdot \\frac{\\partial p}{\\partial a^{<t-1>} }$$\n",
    "\n",
    "The first derivative is a derivative over `tanh`, the second is pretty simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial a^{<t>}}{\\partial a_{<t-1>}} = (1 - tanh^2(p^{<t>})  \\cdot W_{aa} = (1-a^{2<t>})W_{aa} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we refer to our code as:\n",
    "\n",
    "```python\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "```\n",
    "\n",
    "Ufff... derivatives are easy, but take time, patients and energy. No wonder major frameworks have this implemented it *\"under the hood\"* so people can only create computation graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful sources I've used:\n",
    "- https://github.com/go2carter/nn-learn/blob/master/grad-deriv-tex/rnn-grad-deriv.pdf\n",
    "- https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function\n",
    "- http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Backward pass through a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've implemented our backward pass through the network for a single cell. Now we just have to repeat this for our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=loss_compute.png width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b']  += -lr * gradients['db']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    # Initialize gradients as an empty dictionary\n",
    "    gradients = {}\n",
    "    \n",
    "    # Retrieve from cache and parameters\n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "    # Backpropagate through time\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "\n",
    "    \n",
    "    return gradients, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5. Gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our \"flow\" throughout the network is as follows:\n",
    "1. Forward pass throught the network\n",
    "2. Loss computation\n",
    "3. Backward propagation - computing gradients\n",
    "4. Parameter update\n",
    "\n",
    "\n",
    "Between points 3 and 4 we'd be far better of with gradient clipping to prevent \"exploding gradients\", i.e. a situation of numberical overflow that would often produced `NaN`s as gradients take enormously huge number. In order to avoid it we just specify na `maxValue`. If a gradients goes above or below this value we *clip* it, i.e. set to this `maxValue`. Pretty simple:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"clipping.png\" width=900 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    # Unpacking gradient for clarity\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    # clipping to mitigate exploding gradients\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue,+maxValue,gradient) #we overwrite current 'gradient' with clipped one.\n",
    "\n",
    "    # We pack gradient to a ditionary\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    #and return it.\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# III. Creating our RNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Our architecture..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RNNarch.png\" width=900 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "Sampling is the process of generating novel *observation*, in our example new letters that would constitue a made-up bacterial genera name.\n",
    "\n",
    "If our network is trained we can then pass a zero vector $\\vec{0}$ as input hidden state $a^{<0>}$. Then we perform propagation through the first unit of our NN. So we obtain next hidden state $a^{<t+1>}$ and prediction vector $\\hat{y}^{<t+1>}$ that represents the probabilities of each character $i$:\n",
    "\n",
    "\n",
    "$$ a^{<t+1>} = tanh(W_{ax}x^{<t>} + W_{aa}a^{<t>} + b) $$\n",
    "\n",
    "$$ \\hat{y}^{<t+1>}  = softmax( W_{ya}a^{<t+1>} + b_y  )  $$\n",
    "\n",
    "\n",
    "Having computed a vector of probabilities $\\hat{y}^{<t+1>}$ we now perform **sampling** procedure. We do not pick just the highest probability, this would in turn generate the same results each time for a given dataset. We do not want to pick our characters randomly, as results would become random, and all the architecture build would become useless. The key is to select (i.e. *sample*) from our $\\hat{y}^{<t+1>}$ distribution based on that distribution.\n",
    "\n",
    "In other words, we'll pick the index $i$ (remember that we have a one-hot encoded our alphabeth) with the probability encoded by the $i$-th index in $\\hat{y}^{<t+1>}$ matrix. We'll use numpy `random.choice` to achieve this.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "np.random.choice([list of possible values], p = flattened-probability-distribution)\n",
    "\n",
    "```\n",
    "\n",
    "The final step is to **overwrite** the variable `x^{<t+1>}` with our predicted one-hot encoding of selected index $i$ from the previous step. This is represented as red arrow on the picture below:\n",
    "\n",
    "If our network is trained we can then pass a zero vector $\\vec{0}$ as input hidden state $a^{<0>}$. Then we perform propagation through the first unit of our NN. So we obtain next hidden state $a^{<t+1>}$ and prediction $\\hat{y}^{<t+1>}$ that represents the probability that the character indexed by $i$ will become the next character:\n",
    "\n",
    "\n",
    "$$ a^{<t+1>} = tanh(W_{ax}x^{<t>} + W_{aa}a^{<t>} + b) $$\n",
    "\n",
    "$$ \\hat{y}^{<t+1>}  = softmax( W_{ya}a^{<t+1>} + b_y  )  $$\n",
    "\n",
    "\n",
    "Having computed a vector of probabilities $\\hat{y}^{<t+1>}$ we now perform **sampling** procedure. We do not pick just the highest probability, this would in turn generate the same results each time for a given dataset. We do not want to pick our characters randomly, as results would become random, and all the architecture build would become useless. The key is to select (i.e. *sample*) from our $\\hat{y}^{<t+1>}$ distribution based on that distribution.\n",
    "\n",
    "In other words, we'll pick the index $i$ (remember that we have a one-hot encoded our alphabeth) with the probability encoded by the $i$-th index in $\\hat{y}^{<t+1>}$ matrix. We'll use numpy `random.choice` to achieve this.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "np.random.choice([list of possible values], p = flattened-probability-distribution)\n",
    "\n",
    "```\n",
    "\n",
    "The final step is to **overwrite** the variable `x^{<t+1>}` with our predicted one-hot encoding of selected index $i$ from the previous step. This is represented as red arrow on the picture below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=sampling2.png width=900 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue this propagation until we reach end of the line character `\\n`. Then our generation is finished, and we can print our result. If something goes wrong, then we additionally limit ourselves to `50` character limit as indicated by `counter` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- python dictionary mapping each character to an index.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation)\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    # Step 1': Initialize a_prev as zeros\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    \n",
    "    # An empty list of indices, this is the list which will contain the list of indices of the characters to generate \n",
    "    indices = []\n",
    "    \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "    idx = -1 \n",
    "    \n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
    "    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n",
    "    # trained model), which helps debugging and prevents entering an infinite loop. \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Step 2: Forward propagate x using the above equations\n",
    "        a = np.tanh(np.dot(Wax,x)+np.dot(Waa,a_prev)+b)\n",
    "        z = np.dot(Wya,a)+by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        idx = np.random.choice(range(vocab_size), p = y.ravel()) # numpy 'ravel' flattens our list.\n",
    "      \n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[idx] = 1 #this is one hot encoding. All zeros, then set index as '1'.\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "\n",
    "        counter +=1\n",
    "\n",
    "    # if something goes wrong, we'll stop at 50th index.\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a language model\n",
    "\n",
    "### Stochastic Gradient Descent optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model we'll use stochastic gradient descent, meaning we'll loop over one example at a time.\n",
    "\n",
    "Function `optimize` will forward propagate through our network, compute and clip gradients, and update parameters according to provided learning rate. Here we are going to use already implemented functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01,maxGradient=5):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Forward propagate through time \n",
    "    loss, cache =  rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # Backpropagate through time\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip your gradients between -maxGradient (min) and maxGradient (max) \n",
    "    gradients = clip(gradients, maxGradient)\n",
    "    \n",
    "    # Update parameters\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(filename, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, genera_names = 7, vocab_size = vocab_size,learning_rate=0.01,params=None,printing=True):\n",
    "    \"\"\"\n",
    "    Trains the model and generates novel genera names. \n",
    "    \n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    genera_names -- number of bacterial names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    if params==None:\n",
    "        parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    else:\n",
    "        parameters=params\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss)\n",
    "    loss = get_initial_loss(vocab_size, genera_names)\n",
    "    \n",
    "    # Build list of all bacterial names (training examples).\n",
    "    with open(filename) as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples] #we convert names to lowercase and create a list of genera names\n",
    "    \n",
    "    # Shuffle list of all bacterial names\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "\n",
    "        # Define one training example (X,Y) (≈ 2 lines)\n",
    "        index = j % len(examples) # we don't want our index to exceed the number of iterations, so we always refer to a valid example\n",
    "        \n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "            # [None] at the beginning will be interpreted as zero vector by a function `rnn_forward`\n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "            # Y won't have [None] but will be shifted to the right and have end of sentence sign.\n",
    "        \n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = learning_rate)\n",
    "        \n",
    "\n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if (j % 2000==0 and printing==True):\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of genera names to print\n",
    "            for name in range(genera_names):\n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "\n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model('genera.txt', ix_to_char, char_to_ix, num_iterations = 85000,params=None,printing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 32.940970\n",
      "\n",
      "Caryhepherea\n",
      "Xiderlis\n",
      "Daluriphodycapideaestrerous\n",
      "Hyoellochus\n",
      "Agrerodancherus\n",
      "Sardaera\n",
      "Talkita\n",
      "Gonisaia\n",
      "Verodopoda\n",
      "Pydodya\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 25.390203\n",
      "\n",
      "Oelytilon\n",
      "Hropsium\n",
      "Rypesploma\n",
      "Dicocenchomenum\n",
      "Calnima\n",
      "Sestera\n",
      "Hingia\n",
      "Horgopis\n",
      "Helficarmus\n",
      "Sylotes\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 23.936103\n",
      "\n",
      "Salanillus\n",
      "Thraxtnopla\n",
      "Ehndcospira\n",
      "Pyrropotropes\n",
      "Testesyllus\n",
      "Phrbodopibala\n",
      "Lentromingonibactimum\n",
      "Rytticoccus\n",
      "Zylocrocera\n",
      "Dopyora\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 23.759958\n",
      "\n",
      "Obyraptorium\n",
      "Gopediodrida\n",
      "Sardobella\n",
      "Teles\n",
      "Acorospa\n",
      "Decerocolponia\n",
      "Hlosta\n",
      "Nonea\n",
      "Tidrdillla\n",
      "Alacophivintesia\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = model('genera.txt', ix_to_char, char_to_ix, num_iterations = 8000,params=parameters, genera_names=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "\n",
    "Below I show some of the results that you cannot find in Silva database, but for me sound probable and really cool:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nitronella\n",
    "- Sebacter\n",
    "- Vetia\n",
    "- Setinelfonax\n",
    "- Vestaphylococcus\n",
    "- Setonas\n",
    "- Nembacterium\n",
    "- Pioclococclus\n",
    "- Detiptonus\n",
    "- Frreptococcus\n",
    "- Teeutomonas\n",
    "- Fetiphylococcus\n",
    "- Blunna\n",
    "- Alococella\n",
    "- Tantatum\n",
    "- Cublia\n",
    "- Palibacter\n",
    "- Arstrosa\n",
    "- Glymia\n",
    "- Actoboctellibacterium"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
