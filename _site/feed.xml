<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robert Kwapich Homepage</title>
    <description>Robert Kwapich Homepage</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 30 May 2018 12:47:04 -0500</pubDate>
    <lastBuildDate>Wed, 30 May 2018 12:47:04 -0500</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>Bacterial name generator with RNNs</title>
        <description>&lt;p&gt;In this post I’ll cover how to build an RNN from scratch. In accompanying Jupyter notebook I’ll cover how to do this in 1) Python and Numpy (painstaking implementation) and how to do it quickly in 2) KERAS with Tensorflow backend. I’ll present how to construct an RNN unit, then an RNN network. I’ll compute all the derivatives needed for backpropagation &lt;em&gt;‘throught time’&lt;/em&gt;. Finally I’ll use the constructed networks and bacterial &lt;a href=&quot;https://www.arb-silva.de/&quot;&gt;SILVA database&lt;/a&gt; to extract bacterial genera names, and &lt;em&gt;sample&lt;/em&gt;, that is generate novel names through build model.&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;Cover photo source: ... &lt;a target=&quot;_blank&quot; href=&quot;&quot;&gt;[ref].&lt;/a&gt;&lt;/font&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;

&lt;p&gt;For some time I was working on the human gut microbiome, analyzing alterations of compositionality and functional capability in disease (publication in progress). At the same time I was playing with some neural network architectures, like RNNs discussed here, and its ability to create novel sentences or words based on already acquired &lt;em&gt;“knowledge”&lt;/em&gt;. In the era of Next Generation Sequencing (NGS), and especially Whole Metagenome Shotgun (WMS) sequencing it is now possible to study &lt;em&gt;‘microbial dark matter’&lt;/em&gt;, i.e. microbes that were previously uncharacterized, unknown mainly due to the hardships of cultivation or the specific geographic location, like some distant and exotic places (deep seas). I agree with &lt;a href=&quot;http://merenlab.org/2017/06/22/microbial-dark-matter/&quot;&gt;Dr. Murat Eren &lt;/a&gt; that &lt;em&gt;‘microbial dark matter’&lt;/em&gt; is a term wrong on so many levels, and shouldn’t be used… So, I won’t be using it again. I needed this term just to catch your attention.&lt;/p&gt;

&lt;p&gt;Some microbes &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_bacterial_genera_named_after_personal_names&quot;&gt;have been named after its discoverer&lt;/a&gt;, some &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_bacterial_genera_named_after_geographical_names&quot;&gt;after geographical names&lt;/a&gt;, and finally some &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_bacterial_genera_named_after_institutions&quot;&gt;after institutions&lt;/a&gt;. Novel bacterial phyla, genera, species of strains are discovered now desipte limitations in its culturability. This is mainly achieved by computational methods: contig creation, genomic binning and further refinements based on single-copy core genes, redundancy measures etc… I though to myself, why not then come up with some bacterial name created by a neural network, that would create some novel names based on thousands of available genera name?&lt;/p&gt;

&lt;p&gt;Here, I did just that. Since I quite enjoy playing with neural networks, I dediced to come up with this &lt;em&gt;fun project&lt;/em&gt;. Also, I decided also to carry out painstaking computations of backpropagation algorithm through RNNs. It is often neglected, as all major existing frameworks do this automatically for you. It must have been Richard Feynman that said: &lt;em&gt;“You don’t understand something completly if you have not built it yourself”&lt;/em&gt;. If it was not him, who cares, it is still a good advice, and I am going to apply it here.&lt;/p&gt;

&lt;p&gt;I’ll cover &lt;strong&gt;some&lt;/strong&gt; RNN theory, carry out calculations of derivatives, propose RNN architecture, discuss how to train a model, and finally go over how to &lt;em&gt;sample&lt;/em&gt; novel observations from the trained model. Some parts of my discussion may contain brief python code listings, however for a complete interactive examples please refer to &lt;a href=&quot;FILL HERE&quot;&gt;jupyter notebook&lt;/a&gt;. I encourage YOU first to read this post, then follow along jupyter notebook, so you would have had built intuition by the time you read code.&lt;/p&gt;

&lt;h1 id=&quot;rnns&quot;&gt;RNNs&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post I am going to assume you are somewhat familiar with neural networks (NNs), since RNNs are variations on standard architectures. First let me show you a nice RNN block diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/_images/bacterial_names/RNNcell.png&quot; alt=&quot;a Basic RNN cell&quot; /&gt;&lt;/p&gt;
&lt;font size=&quot;2&quot;&gt;Imageo source: Andrew Ng&#39;s &quot;Sequence Models&quot; &lt;a target=&quot;_blank&quot; href=&quot;https://www.coursera.org/learn/nlp-sequence-models/&quot;&gt;[ref].&lt;/a&gt;&lt;/font&gt;

&lt;p&gt;RNN can work on either characters or words. In this project I am going to generate words, so RNN model will inevitabely work on characters. Let me briefly describe what each notation element stands for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\( x^{&lt;t&gt;} \\) is an `t-th` element of sequence \\( X \\). Input sequence \\( X = \{ x^{&amp;lt;1&amp;gt;},x^{&amp;lt;2&amp;gt;},...,x^{&lt;t-1&gt;},x^{&lt;t&gt;}  \} \\),&lt;/t&gt;&lt;/t-1&gt;&lt;/t&gt;&lt;/li&gt;
  &lt;li&gt;\( W_{aa} \), \( W_{ax} \), \( W_{ya} \): the weights of our RNN cell,&lt;/li&gt;
  &lt;li&gt;\( a^{&lt;t-1&gt;} \\), \\( a^{&lt;t&gt;} \\): are *hidden state* activation from previous RNN cell (`t-1`) and for the next RNN cell (`t`),&lt;/t&gt;&lt;/t-1&gt;&lt;/li&gt;
  &lt;li&gt;\( \hat{y}^{&lt;t&gt;} \\): is a prediction, a probability of a given character at a time-step `t`, usually computed by a `softmax` function as we operate on one-hot encodings,&lt;/t&gt;&lt;/li&gt;
  &lt;li&gt;\( b_a \): is a bias term&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The rest is a set of additions and multiplications. Essentialy the whole architecture is build upon this cell by means of repetition:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/_images/bacterial_names/RNNarch.png&quot; alt=&quot;a basic RNN architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Forward pass through this network needs to compute \( \hat{Y} \) (prediction) vectors, hidden states \( a \), and store some of its computing results in &lt;em&gt;cache&lt;/em&gt; that is needed for backpropagation. After forward pass, for our model we’ll use a simple cross-entropy function as a loss. For a single timestep it is defined as:&lt;/p&gt;

&lt;p&gt;\[  L_t(y_t,\hat{y}_t) = - y_tlog(\hat{y}_t) \]&lt;/p&gt;

&lt;p&gt;and for an entire sequence cost is just a sum of these values:&lt;/p&gt;

&lt;p&gt;\[ L(y,\hat{y}) = - \Sigma_t y_tlog(\hat{y}_t) \]&lt;/p&gt;

&lt;h2 id=&quot;one-hot-encodings&quot;&gt;One-hot encodings&lt;/h2&gt;

&lt;p&gt;Input sequence at a given timestep &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;: \( x^{&lt;t&gt;} \\) must come in a handy format for computations.&lt;/t&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;padding&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parameters-initialization&quot;&gt;Parameters initialization&lt;/h2&gt;

&lt;h2 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h2&gt;

&lt;p&gt;Backpropagation in an RNN network is complicated, but not that hard. Essentialy it is often said that deriving these computations by hand is the most &lt;em&gt;‘complex’&lt;/em&gt; thing in neural networks. Since many frameworks outsource this for their users, it has become neglected - and I don’t blame anybody, it really is convenient. For a matter of practice however, in this section I am going to carry out all necessary calculations for the backpropagation through our RNN architecture.&lt;/p&gt;

&lt;p&gt;Since these calculations take much space, and would hide the &lt;em&gt;“big picture”&lt;/em&gt;, please follow &lt;a href=&quot;/helper_posts/RNN_backprop.markdown&quot;&gt;this link&lt;/a&gt; for a complete set of computations. Below I reproduce the final derivative terms for convenience:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;test&lt;/li&gt;
  &lt;li&gt;test&lt;/li&gt;
  &lt;li&gt;test&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;resources&quot;&gt;Resources&lt;/h1&gt;

</description>
        <pubDate>Mon, 21 May 2018 00:00:00 -0500</pubDate>
        <link>/2018/05/21/RNN-bacteria-name-generator/</link>
        <guid isPermaLink="true">/2018/05/21/RNN-bacteria-name-generator/</guid>
        
        
        <category>Data-Science</category>
        
      </item>
    
      <item>
        <title>Simple anomaly detection system</title>
        <description>&lt;p&gt;In this post I’ll quickly go over a simple &lt;em&gt;anomaly detection system&lt;/em&gt;: what is it, what are its motivations and how to build it. I’ll use an example dataset of hypothyroid patients to look for anomalies. You can read this post, &lt;a target=&quot;_blank&quot; href=&quot;https://github.com/vaxherra/vaxherra.github.io/blob/master/_files/anomaly_detection/anomaly.ipynb&quot;&gt;&lt;b&gt;&lt;u&gt;follow along a jupyter notebook &lt;/u&gt;&lt;/b&gt;&lt;/a&gt; for code snippets or &lt;a target=&quot;_blank&quot; href=&quot;/_files/anomaly_detection/anomaly.ipynb&quot;&gt;&lt;b&gt;&lt;u&gt;download&lt;/u&gt;&lt;/b&gt;&lt;/a&gt; it for a local use. I obtained the dataset from &lt;a href=&quot;http://odds.cs.stonybrook.edu/thyroid-disease-dataset/&quot;&gt;&lt;em&gt;Outlier Detection Dataset&lt;/em&gt; website&lt;/a&gt;, but you can download it &lt;a target=&quot;_blank&quot; href=&quot;/_files/anomaly_detection/thyroid.mat&quot;&gt;&lt;b&gt;&lt;u&gt;here&lt;/u&gt;&lt;/b&gt;&lt;/a&gt; if something happens to the source.&lt;/p&gt;

&lt;font size=&quot;1&quot;&gt;Cover photo source: a cropped fragment from Pieter&#39;s Bruegel the Elder &lt;i&gt;&#39;Netherlandish Proverbs&#39;&lt;/i&gt;. This painting is a list of proverbs and idioms, and this particular fragments means &quot;To be able to tie even the devil to a pillow&quot;, i.e. Obstinacy overcomes everything &lt;a target=&quot;_blank&quot; href=&quot;https://en.wikipedia.org/wiki/Netherlandish_Proverbs&quot;&gt;[ref].&lt;/a&gt;&lt;/font&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Anomaly detection&lt;/em&gt; is really an &lt;em&gt;outlier&lt;/em&gt; detection problem. Given a certain set of observations we want to build some kind of understanding (model), so given a new example we can determine if it matches our previous observations or is not coming from the same distribution. An anomaly is different from standard classification problem due to the nature of our dataset. Often in anomaly detection we are given a dataset with &lt;em&gt;skewed classes&lt;/em&gt;, i.e. we have much more &lt;em&gt;negative&lt;/em&gt; (non-anomalous) that positive (anomalous) example. You can imagine a highly efficient production system, where most of our products are of good, acceptable quality, and are ready to be sold. Sometimes, however, we report a faulty product that cannot go into the market. Imagine that out of 100 thousand examples only 125 were &lt;em&gt;positive&lt;/em&gt; (anomalous). So only 0.125% of examples are &lt;em&gt;positive&lt;/em&gt;. Which is good for our production system, but makes it harder to use state of the art, ‘data-hungry’ machine learning algorithms based on neural nets (NN). NNs would probably not be able to properly learn what an &lt;em&gt;anomaly&lt;/em&gt; means. We often say, there is an infinite number of ways something might go wrong, but usually, there is a limited number of ways to do something properly.&lt;/p&gt;

&lt;p&gt;Let’s say that we can automatically or semi-automatically collect some features from our products. Given the sheer number of features, it is often not possible to manually make sure our system or product it’s working properly. But wait. Didn’t I say that our example is going to include hypothyroid patients? Yes, indeed. For biological problems, this might be even harder, as biological systems usually are characterized by huge variability.&lt;/p&gt;

&lt;h3 id=&quot;hypothyroid-dataset-overview&quot;&gt;Hypothyroid dataset overview&lt;/h3&gt;
&lt;p&gt;I’ve already mentioned that our dataset is related to the hypothyroid disease. I’ve obtained this dataset from &lt;a href=&quot;http://odds.cs.stonybrook.edu/thyroid-disease-dataset/&quot;&gt;Outlier Detection Datasets&lt;/a&gt; website. It comprises of 3772 subjects, only 93 subjects are characterized as hypothyroid (i.e. positive, anomalous) ( \( 2.5\% \) ). Each subject is attributed with six real-value features that we must use to build a model and predict whether it is or might be hypothyroid patient or not. In addition data has \( \hat{Y} \in {0,1 } \) labels, that state the &lt;em&gt;‘ground truth’&lt;/em&gt;, so we know whether a certain subject was actually hypothyroid or not. In fact, if you read the data described in the provided link, you can see that dataset merges two classes (normal and subnormal functioning) into one “normal class”. Unfortunately, features are not named, so we actually don’t know what each of them represents. But for learning purposes, this is enough. Ok, let’s proceed to formulate a problem for our dataset.&lt;/p&gt;

&lt;h3 id=&quot;problem-formulation&quot;&gt;Problem formulation&lt;/h3&gt;
&lt;p&gt;Suppose we are working as a (clinical) data analyst for some medical organization close to a GP. One day we are given a moderate in size dataset of our patients. The staff has been collecting some six quantifiable features, be it symptoms or tests, that are indicative (to a limited extent) or suggest a hypothyroidism - “a condition in which the thyroid gland doesn’t produce enough thyroid hormone”. Our data also has a label stating whether a patient actually was hypothyroid.&lt;/p&gt;

&lt;p&gt;We are not running a thyroid diagnosis and treatment center, but are a part of first contact team. For doctors, it would be nice to build some model that detects anomalies in patients, so further efforts might focus on specific organs or targeted diagnostics. Say, patient visits a GP, goes over symptoms and has a set of basic tests and measurements. Given our data, we can’t really tell whether this patient is or is not hypothyroid, but rather whether there is some significant abnormality in a set of his results that, given small historical data, we may want to send him or her to check specifically for thyroid.&lt;/p&gt;

&lt;p&gt;Our dataset is relatively small and skewed as most patients do not have hypothyroidism. This limits us directly in using “state of the art”, everyone’s favorite neural net classification system. However, we can try working with this data and construct an anomaly detection system.&lt;/p&gt;

&lt;h2 id=&quot;building-and-testing-a-model&quot;&gt;Building and testing a model&lt;/h2&gt;

&lt;p&gt;The idea is simple. We model each feature of our dataset by a multivariate Gaussian distribution: compute mean matrix ( \( \mu \)  ) and covariance matrix ( \( \Sigma \) ) on a training set:&lt;/p&gt;

&lt;p&gt;\[ \mu = \frac{1}{m} \Sigma^m_i x^{(i)} \]&lt;/p&gt;

&lt;p&gt;\[ \Sigma = \frac{1}{m} \Sigma^m_i (x^{(i)}-\mu)\cdot(x^{(i)}-\mu)^T \]&lt;/p&gt;

&lt;p&gt;Mean matrix \( \mu \) contains a mean value for each feature &lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt;. Be sure to distinguish a summation sigma \( \Sigma_i^m \) from a covariance matrix sigma \( \Sigma \). This can be misleading, but just a bit. Then, given a new example \( x^{i}_1,…,x^{i}_n \) (with &lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt; features) we can compute its probability as defined by Multivariate Gaussian Distribution:&lt;/p&gt;

&lt;p&gt;\[ f_x(x^{i}) = \frac{  exp(- \frac{1}{2} (x^i-\mu)^T  \Sigma^{-1} (x^i-\mu)  ) }{  \sqrt{2\pi^n  | \Sigma |    }} \]&lt;/p&gt;

&lt;p&gt;Having computed probability of a given, new example, we can then decide based on that single number whether it is coming from our distribution or not. However, we should also determine a probability threshold \( \epsilon \) below which we consider an example as &lt;em&gt;anomalous&lt;/em&gt;. But before we do that, we have to intelligently split our dataset.&lt;/p&gt;

&lt;p&gt;We need to reasonably split the dataset for training, cross-validation and model testing. The idea is that for model training we use only negative examples, i.e. non-anomalous. Since we want to model each feature with a Gaussian distribution, it would be appropriate to &lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt; our model on negative examples and use a smaller portion of positive examples for hyperparameter tuning (\( \epsilon \) ) and model testing.&lt;/p&gt;

&lt;p&gt;Table below shows how one might approach it for our hypothyroid dataset:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Sets&lt;/th&gt;
      &lt;th&gt;# Negative examples&lt;/th&gt;
      &lt;th&gt;# Positive examples&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Train set&lt;/td&gt;
      &lt;td&gt;2999&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cross-validation set&lt;/td&gt;
      &lt;td&gt;340&lt;/td&gt;
      &lt;td&gt;46&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Test set&lt;/td&gt;
      &lt;td&gt;340&lt;/td&gt;
      &lt;td&gt;47&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So train set is used to compute mean matrix \( \mu \) and covariance matrix \( \Sigma \). Cross-validation set is used to set an \( \epsilon \) probability threshold. How do we do this? We loop over an arbitrary number of possible probabilities, say one thousand or million points between the minimum and maximum probability obtained from a cross-validation set. Since our classes (anomalous vs non-anomalous) are skewed (there is disproportionately more negative examples that positives) we cannot use simple &lt;em&gt;accuracy&lt;/em&gt; based on frequency, i.e. how many times our classifier was correct. Imagine that we predict \( y=0 \) all the time. Since the majority of our subjects are non-anomalous this would actually classify wrongly  93 out of 3772 and that would give us 97.53% accuracy. But this is just wrong. We do want to catch some anomalies if they likely occur, even if we might be wrong about them, as it is essentially better to send a non-hypothyroid patient for additional screening than to neglect actual hypothyroidism in a patient. We’d be happier even if we traded some of the precision for a recall.&lt;/p&gt;

&lt;p&gt;Thus, we must operate using &lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;F1&lt;/code&gt; score&lt;/a&gt; based on measure of precision and recall calculated using true positives (&lt;code class=&quot;highlighter-rouge&quot;&gt;tp&lt;/code&gt;), false positives (&lt;code class=&quot;highlighter-rouge&quot;&gt;fp&lt;/code&gt;) and false negatives (&lt;code class=&quot;highlighter-rouge&quot;&gt;fn&lt;/code&gt;):&lt;/p&gt;

&lt;p&gt;\[ F_1 = \frac{2 \cdot precision \cdot recall}{precision + recall} \]&lt;/p&gt;

&lt;p&gt;\[ precision =  \frac{tp}{tp+fp} \]&lt;/p&gt;

&lt;p&gt;\[ recall = \frac{tp}{tp+fn}  \]&lt;/p&gt;

&lt;p&gt;Just a reminder. True positive refer to anomalous examples. If, for a given \( \epsilon \) over which we are iterating the computed probability \( f_x^i &amp;lt; \epsilon \) and our ground truth label \( y=1 \) then we can count an example as true positive. False negative would occur for the same true label, but we’d observe \( f_x^i &amp;lt;&amp;gt; \epsilon \). And finally a false positive would produce a small probability, below-given threshold \( f_x^i &amp;lt; \epsilon \), but in reality, the truth label says it’s a non-anomalous example \( y=0 \).&lt;/p&gt;

&lt;p&gt;Iterating over a set of \( epsilon \) values we choose the one that minimizes errors, i.e. maximizes our F1 score. We then use this \( \epsilon \) on the third portion of our data - &lt;em&gt;test set&lt;/em&gt; and compute final precision, recall, and F1 scores.&lt;/p&gt;

&lt;p&gt;If you follow &lt;a target=&quot;_blank&quot; href=&quot;https://github.com/vaxherra/vaxherra.github.io/blob/master/_files/anomaly_detection/anomaly.ipynb&quot;&gt;&lt;b&gt;&lt;u&gt;jupyter notebook&lt;/u&gt;&lt;/b&gt;&lt;/a&gt; for this post, then you observed that our final F1 score is \( F1 \approx 0.73 \) with \( precision \approx 0.58 \) and  \( recall \approx 0.96 \). Is this good?&lt;/p&gt;

&lt;h2 id=&quot;final-comment&quot;&gt;Final comment&lt;/h2&gt;

&lt;p&gt;Our model has a high recall, which means we very well identify all anomalies. So we are “good” at catching anomalies in patients when they occur, however when you look at our precision score, it is low. Precision essentially measures how well (precisely) our model identifies anomalies. I.e. our low score indicates that model will produce an “alert” even if some patient might not be hypothyroid. Is this good? Given the skewed dataset, and serious lack of positive examples I’d say it is helpful. When a new patients comes in, we perform a given set of basic tests, and a model predicts that he or she might be hypothyroid, we just sent this patient for a detailed set of tests that are more precise (or sensitive). This is not a serious decision on performing a complicated operation relying on our system. But at the same time, we make sure that we are catching \( \approx 96\% \) of anomalies, as our recall score indicates. Imagine you have a huge turnouver of patients in a first-contact clinic. Spending less time on manually looking at results and wondering whether they might be indicative of malfunctioning thyroid saves time for a GP. Also, at certain situations thus might &lt;em&gt;at hoc&lt;/em&gt; suggest hypothyroid disease, and drive a GP to ask questions specific to symptoms (feeling increasingly tired, have dry skin, constipation and weight gain) or directly send a patient for a sensitive TSH test.&lt;/p&gt;

&lt;p&gt;Oftentimes building a model and estimating accuracy is not enough. We need a model interpretation for a particular use. We have to analyze the nature of the problem and think about desired output. Here we have a standard precision and recall tradeoff, in which we are far better of with higher recall.&lt;/p&gt;
</description>
        <pubDate>Tue, 15 May 2018 00:00:00 -0500</pubDate>
        <link>/2018/05/15/anomaly-detection/</link>
        <guid isPermaLink="true">/2018/05/15/anomaly-detection/</guid>
        
        
        <category>Data-Science</category>
        
      </item>
    
      <item>
        <title>Homepage Launches</title>
        <description>&lt;p&gt;Welcome to my homepage!&lt;/p&gt;

&lt;p&gt;I am Robert Kwapich. I studied medical physics and obtained a Master’s degree from the University of Silesia in Katowice, Poland. I am currently working in the domain of bioinformatics and data-science as a Research Assistant in the US of A. I enjoy playing with machine learning and deep learning methods for data science in biology and medicine. Read more about me on the &lt;a href=&quot;/&quot;&gt;main page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The intent of this website is double-fold. I want to present some of my ideas, thoughts and inspirations to the world in the form of blog posts. They can range from cinema, paintings, music, philosophy to physics, biology, computer science and some branches of artificial intelligence. The second pillar of my website is to present my actual work and implemented ideas in code. They can serve as my portfolio, or as a reference for somebody interested. Each post has a comment box hosted on &lt;strong&gt;disqus&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;I plan to publish every 6-8 weeks in several categories: blog, data-science and bioinformatics. Right now you can navigate to my first post about &lt;a href=&quot;_posts/2018-05-11-anomaly-detection&quot;&gt;simple anomaly detection&lt;/a&gt; or to the &lt;a href=&quot;/&quot;&gt;main page&lt;/a&gt; and &lt;em&gt;‘about me’&lt;/em&gt; tab.&lt;/p&gt;

&lt;p&gt;Feel free to say hello in the comment!&lt;/p&gt;
</description>
        <pubDate>Sat, 12 May 2018 00:00:00 -0500</pubDate>
        <link>/2018/05/12/launch/</link>
        <guid isPermaLink="true">/2018/05/12/launch/</guid>
        
        
        <category>Blog</category>
        
      </item>
    
  </channel>
</rss>
