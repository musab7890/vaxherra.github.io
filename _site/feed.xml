<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robert Kwapich Homepage</title>
    <description>Robert Kwapich Homepage</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 01 Oct 2018 18:03:22 -0400</pubDate>
    <lastBuildDate>Mon, 01 Oct 2018 18:03:22 -0400</lastBuildDate>
    <generator>Jekyll v3.7.4</generator>
    
      <item>
        <title>Bacterial name generator with RNNs</title>
        <description>&lt;p&gt;In this post I’ll cover how to build a Recurrent Neural Network (RNN) from scratch, as well as using existing Deep Learning (DL) frameworks to create new bacterial names from thousands of publicly available annotations.&lt;/p&gt;

&lt;p&gt;In accompanying Jupyter notebooks I’ll cover how to do this in:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Python and Numpy (painstaking implementation) &lt;a href=&quot;https://github.com/vaxherra/vaxherra.github.io/blob/master/_files/bacterial_names/RNNs.ipynb&quot;&gt;[see here]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;KERAS with Tensorflow backend (quick implementation) - &lt;a href=&quot;https://github.com/vaxherra/vaxherra.github.io/blob/master/_files/bacterial_names/RNNs_KERAS.ipynb&quot;&gt;[see here]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I assume you have some knowledge of standard neural networks, are familiar with the ideas of propagation and backpropagation, loss and cost functions. I’ll present how to construct an RNN unit, then an RNN network. I’ll compute all the derivatives needed for backpropagation &lt;em&gt;‘throught time’&lt;/em&gt;. Finally I’ll use the constructed networks and bacterial &lt;a href=&quot;https://www.arb-silva.de/&quot;&gt;SILVA database&lt;/a&gt; to extract bacterial genera names, and &lt;strong&gt;sample&lt;/strong&gt; - generate novel names through our model.&lt;/p&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;

&lt;p&gt;For some time I was working on the human gut microbiome, analyzing alterations of compositionality and functional capability in disease (publication in progress). At the same time I was playing with some neural network architectures, like RNNs discussed here, and its ability to create novel sentences or words based on already acquired &lt;em&gt;“knowledge”&lt;/em&gt;. In the era of Next Generation Sequencing (NGS), and especially Whole Metagenome Shotgun (WMS) sequencing it is now possible to study &lt;em&gt;‘microbial dark matter’&lt;/em&gt;, i.e. microbes that were previously uncharacterized, unknown mainly due to the hardships of cultivation or the specific geographic location, like some distant and exotic places (deep seas). I agree with &lt;a href=&quot;http://merenlab.org/2017/06/22/microbial-dark-matter/&quot;&gt;Dr. Murat Eren &lt;/a&gt; that &lt;em&gt;‘microbial dark matter’&lt;/em&gt; is a term wrong on so many levels. As “&lt;em&gt;The dark matter in physics is a place holder for a not-yet-characterized form of matter that is distinct from ordinary matter&lt;/em&gt;”, whereas undiscovered microbes are just… uncharacterized. Hence, I won’t be using this term again.&lt;/p&gt;

&lt;p&gt;Some microbes &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_bacterial_genera_named_after_personal_names&quot;&gt;have been named after its discoverer&lt;/a&gt;, some &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_bacterial_genera_named_after_geographical_names&quot;&gt;after geographical names&lt;/a&gt;, and finally some &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_bacterial_genera_named_after_institutions&quot;&gt;after institutions&lt;/a&gt;. Novel bacterial phyla, genera, species of strains are discovered now desipte limitations in its culturability. This is mainly achieved by computational methods: contig creation, genomic binning and further refinements based on single-copy core genes, redundancy measures etc… I though to myself, why not then come up with some bacterial name created by a neural network, that would create some novel names based on thousands of available genera name?&lt;/p&gt;

&lt;p&gt;Here, I did just that. Since I quite enjoy playing with neural networks, I decidced to come up with this fun mini project. I decided also to carry out painstaking computations of backpropagation algorithm through RNNs. It is often neglected, as all existing frameworks do this automatically for you. It must have been Richard Feynman that said: &lt;em&gt;“You don’t understand something completly if you have not built it yourself”&lt;/em&gt;. If it was not him who said it, who cares? it is still a good advice, and I am going to apply it here.&lt;/p&gt;

&lt;p&gt;I’ll cover &lt;strong&gt;some&lt;/strong&gt; RNN theory, carry out calculations of derivatives, propose RNN architecture, discuss how to train a model, and finally go over how to &lt;em&gt;sample&lt;/em&gt; novel observations from the trained model. Some parts of my discussion may contain brief python code listings, however for a complete interactive examples please refer to the &lt;strong&gt;jupyter notebooks&lt;/strong&gt; (links above). I encourage you to first read this post, then follow along jupyter notebook of your choice. That way you would have had built intuition by the time you read code implementations which are not as abundant in explanations. And as I mentioned earlier, feel free to chose a simple implementation with KERAS or a more complicated implementation with python.&lt;/p&gt;

&lt;h1 id=&quot;input-dataset&quot;&gt;Input dataset&lt;/h1&gt;

&lt;p&gt;For this project I am going to use a &lt;a href=&quot;https://www.arb-silva.de/&quot;&gt;SILVA database&lt;/a&gt;  “&lt;em&gt;A comprehensive on-line resource for quality checked and aligned ribosomal RNA sequence data.&lt;/em&gt;”. In fact I’ll only be focusing on taxonomy annotations for bacterial genera. Currently the latest release is numbered 132 and the &lt;code class=&quot;highlighter-rouge&quot;&gt;gzipped&lt;/code&gt; file can be downloaded directly &lt;a href=&quot;https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/SILVA_132_LSUParc_tax_silva.fasta.gz&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The previously mentioned &lt;code class=&quot;highlighter-rouge&quot;&gt;jupyter&lt;/code&gt; notebooks provide code, a set of simple unix, &lt;code class=&quot;highlighter-rouge&quot;&gt;awk&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;sed&lt;/code&gt; tricks piped together, showing how to reformat this &lt;code class=&quot;highlighter-rouge&quot;&gt;.fasta&lt;/code&gt; file to come up with list of genera. The formatted file can be &lt;a href=&quot;/_files/bacterial_names/genera.txt&quot;&gt;downloaded here&lt;/a&gt;. It contains &lt;strong&gt;7901&lt;/strong&gt; genera names that are going to be used for our model training.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; head &lt;span class=&quot;nt&quot;&gt;-10&lt;/span&gt; genera.txt

Abelmoschus
Abies
Abiotrophia
Abolboda
Abortiporus
Abrahamia
Abroma
Abrus
Absidia
Abuta&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For our purposes the input dataset is converted to lowercase, as it is better that our model doesn’t learn something that is not useful for our task at hand.&lt;/p&gt;

&lt;h1 id=&quot;rnns&quot;&gt;RNNs&lt;/h1&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;For an extensive reading on the topic I direct you to the &lt;em&gt;resources&lt;/em&gt; section at the end of this post. If you are already familiar with Recurrent Neural Networks  (RNNs), skip this section, as it only reviews what is special with RNNs as opposed to &lt;em&gt;feedforward networks&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Standard neural models are often referred to as &lt;em&gt;feedforward networks&lt;/em&gt; as each input, i.e. a data point or an image is treated as separate entity. The model has no “&lt;em&gt;memory”&lt;/em&gt;, and is not a particularly good fit for processing sequence of data. An RNN is used to process sequences of information through iteration of each element of the data, like time-points, words or characters. It tries to &lt;em&gt;keep memory&lt;/em&gt; of what came before.&lt;/p&gt;

&lt;p&gt;Architecture of an RNN maintains an additional parameter, so called &lt;em&gt;state&lt;/em&gt; that preserves information corresponding to what has been observed to far by the network. This additional parameter is really another variable retained from processing previous state. In our example in this post, the &lt;em&gt;‘state’&lt;/em&gt; corresponds to a word. This vague definition allows to construct many variations of RNNs, but for the purpose of this rather simplistic problem at hand, we’ll consider the basic RNN architecture.&lt;/p&gt;

&lt;h2 id=&quot;model-overview&quot;&gt;Model Overview&lt;/h2&gt;
&lt;p&gt;In this post I am going to assume you are somewhat familiar with neural networks (NNs), since RNNs are variations on standard architecture. First, let me show you a nice RNN block diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/_images/bacterial_names/RNNcell.png&quot; alt=&quot;a Basic RNN cell&quot; /&gt;&lt;/p&gt;
&lt;font size=&quot;2&quot;&gt;Image source: Andrew Ng's &quot;Sequence Models&quot; &lt;a href=&quot;https://www.coursera.org/learn/nlp-sequence-models/&quot;&gt;[ref].&lt;/a&gt;&lt;/font&gt;

&lt;p&gt;In this project I am going to generate words, so RNN model will inevitably work on characters. Let me briefly describe what each notation element stands for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;\( x^{&amp;lt;t&amp;gt;} \) is an &lt;code class=&quot;highlighter-rouge&quot;&gt;t-th&lt;/code&gt; element of sequence \( X \), i.e. a character. Input sequence has the formula \( X = { x^{&amp;lt;1&amp;gt;},x^{&amp;lt;2&amp;gt;},…,x^{&amp;lt;t-1&amp;gt;},x^{&amp;lt;t&amp;gt;}  } \),&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\( W_{aa} \), \( W_{ax} \), \( W_{ya} \): the weights of our RNN cell,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\( a^{&amp;lt;t-1&amp;gt;} \), \( a^{&amp;lt;t&amp;gt;} \): are &lt;em&gt;hidden state&lt;/em&gt; activations from a previous RNN cell (&lt;code class=&quot;highlighter-rouge&quot;&gt;t-1&lt;/code&gt;) and for the next RNN cell (&lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;),&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\( \hat{y}^{&amp;lt;t&amp;gt;} \): is a prediction, a probability of a given character at a time-step &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;, usually computed by a &lt;code class=&quot;highlighter-rouge&quot;&gt;softmax&lt;/code&gt; function as we operate on one-hot encodings,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\( b_a \): is a bias term&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The rest is a set of additions and multiplications. Essentially the whole architecture is build upon this cell by means of repetition of RNN cells:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/_images/bacterial_names/RNNarch.png&quot; alt=&quot;a basic RNN architecture&quot; /&gt;&lt;/p&gt;
&lt;font size=&quot;2&quot;&gt;Image source: Andrew Ng's &quot;Sequence Models&quot; &lt;a href=&quot;https://www.coursera.org/learn/nlp-sequence-models/&quot;&gt;[ref].&lt;/a&gt;&lt;/font&gt;

&lt;p&gt;Forward pass through this network needs to compute \( \hat{Y} \in ( \hat{Y}^{&amp;lt;1&amp;gt;}, … , \hat{Y}^{&amp;lt;T_x&amp;gt;} ) \) prediction vectors, hidden states \( a \), and store some of its computing results in &lt;em&gt;cache&lt;/em&gt; that is needed for backpropagation. After forward pass, for our model we’ll use a simple cross-entropy function as our model loss. For a single timestep it is defined as:&lt;/p&gt;

&lt;p&gt;\[  L_t(y_t,\hat{y}_t) = - y_t log(\hat{y}_t) \]&lt;/p&gt;

&lt;p&gt;and for an entire sequence, the cost function is just a sum of loss values:&lt;/p&gt;

&lt;p&gt;\[ L(y,\hat{y}) = - \Sigma_t y_tlog(\hat{y}_t) \]&lt;/p&gt;

&lt;h2 id=&quot;parameters-initialization&quot;&gt;Parameters initialization&lt;/h2&gt;

&lt;p&gt;The model, as shown on the image above, has many weights which need to be somewhat initialized before we begin training this network. These parameters are: \( W_{aa},W_{ax}, W_{ya},b,b_y \). As in any standard neural network setting these values to zero would not yield any “learning” results, as each pass and backpropagation would not adjust these weights. For this simplistic example a random initialization, between &lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;0.01&lt;/code&gt; would suffice for &lt;code class=&quot;highlighter-rouge&quot;&gt;W&lt;/code&gt; parameters, and biases &lt;code class=&quot;highlighter-rouge&quot;&gt;b&lt;/code&gt; could be initialized to zeros.&lt;/p&gt;

&lt;h2 id=&quot;forward-propagation&quot;&gt;Forward propagation&lt;/h2&gt;

&lt;h3 id=&quot;forward-propagation-through-an-rnn-cell&quot;&gt;Forward propagation through an RNN cell&lt;/h3&gt;
&lt;p&gt;Forward propagation is a set of repeated propagations through the RNN cell. Each RNN cell takes as input a set of parameters:
 \( W_{aa},W_{ax}, W_{ya},b,b_y \), previous state \( a_{prev}\) and current time-stamp, which is our character at particular position \( x^{&amp;lt;t&amp;gt;}\) (refer to the RNN architecture image above).&lt;/p&gt;

&lt;p&gt;Each RNN cell returns the next hidden state \( a^{&amp;lt;t+1 &amp;gt;} \) computed as an activation function, for example &lt;code class=&quot;highlighter-rouge&quot;&gt;tanh&lt;/code&gt; applied to linear combination of weights and parameters, and current probability computed as a &lt;code class=&quot;highlighter-rouge&quot;&gt;softmax&lt;/code&gt; (an activation function used for probability prediction) from current hidden state, and appropriate weights.&lt;/p&gt;

&lt;p&gt;\[ a^{ &amp;lt;t&amp;gt;}  = tanh( W_{ax}x^{ &amp;lt;t&amp;gt;}  + W_{aa}a^{&amp;lt;t-1&amp;gt;}+b_a   )  \]
  \[ y^{ &amp;lt;t&amp;gt;}  = softmax( W_{ya}a^{ &amp;lt;t&amp;gt;}  +b_y   )  \]&lt;/p&gt;

&lt;h3 id=&quot;forward-propagation-through-a-network-of-rnn-cells&quot;&gt;Forward propagation through a network (of RNN cells)&lt;/h3&gt;
&lt;p&gt;I am repeating myself here, but for the sake of clarity: a forward pass through the network is just a repetition of single blocks, repeated \( T_x \) times (the length of the word).&lt;/p&gt;

&lt;p&gt;As a result of this repeated forward propagation we should obtain \( \hat{Y} \), a vector of predictions (&lt;code class=&quot;highlighter-rouge&quot;&gt;softmax&lt;/code&gt; probabilities), vector \(a \) of hidden states (from first to the last character). Our loss function is a standard cross-entropy function implemented as (for a single time-stamp):&lt;/p&gt;

&lt;p&gt;\[  L_t(y_t,\hat{y}_t) = - y_tlog(\hat{y}_t)  \]&lt;/p&gt;

&lt;p&gt;and for an entire RNN network architecture:&lt;/p&gt;

&lt;p&gt;\[ L(y,\hat{y}) = - \Sigma_t y_tlog(\hat{y}_t) \]&lt;/p&gt;

&lt;h2 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h2&gt;

&lt;p&gt;Backpropagation in an RNN network is complicated, but not that hard. Essentially it is often said that deriving these computations by hand is the most &lt;em&gt;‘complex’&lt;/em&gt; thing in neural networks. Since many frameworks outsource this for their users, it has become neglected - and I don’t blame anybody, it is really convenient. For a matter of practice however, in this section I am going to carry out all necessary calculations for the backpropagation through our RNN architecture.&lt;/p&gt;

&lt;p&gt;If you don’t feel like following these simple derivatives, skip to &lt;em&gt;‘Backpropagation summary’&lt;/em&gt; paragraph, where all necessary derivatives are computed.&lt;/p&gt;

&lt;h3 id=&quot;backpropagation-through-a-cell&quot;&gt;Backpropagation through a cell&lt;/h3&gt;

&lt;p&gt;Our model could be summarized with these three equations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;\( L_t = - y^{&amp;lt;t&amp;gt;} log(\hat{y}^{&amp;lt;t&amp;gt;}) \)&lt;/li&gt;
  &lt;li&gt;\( \hat{y}^{&amp;lt;t&amp;gt;} = softmax ( W_{ya} a^{&amp;lt;t&amp;gt;} + b_y ) = softmax(z) \)&lt;/li&gt;
  &lt;li&gt;\( a^{&amp;lt;t&amp;gt;} = tanh  (W_{ax} x^{&amp;lt;t&amp;gt;} + W_{aa}a^{&amp;lt;t-1&amp;gt;} + b_a) \)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It would also help to know how activation functions are defined:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;\( softmax(z_j) =  \frac{e^z_j}{\Sigma^K_k e^z_k}\) where \(j=1,2,…k,k+1,…,K \)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\( tanh(z) = \frac{sinh(z)}{cosh(z)} =  \frac{e^z - e^{-z}}{e^z + e^{-z}} \)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these we can calculate all the necessary derivatives. These calculations take considerate amount of space, so I put them under the link below:&lt;/p&gt;
&lt;h3 id=&quot;backpropagation-calculations-link-&quot;&gt;&lt;a href=&quot;/statics/RNNbackprop&quot;&gt;Backpropagation calculations [Link] » &lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;If you followed the above link for calculations of backpropagation derivatives, then you already know where these values below come from, if not - then it’s not really necessary for you to follow that.&lt;/p&gt;

&lt;p&gt;Backpropagation through a single cell - final derivatives:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(  \frac{\partial L_t}{\partial W_{ya}}  =  (\hat{y}^{&amp;lt;t_l&amp;gt;} -y^{&amp;lt;t_l&amp;gt;} ) \cdot a^{&amp;lt;t&amp;gt;}  \)&lt;/li&gt;
  &lt;li&gt;\( \frac{\partial L_t}{\partial b_{y}} = \frac{\partial L_t}{\partial \hat{y}^{&amp;lt;t&amp;gt;}} \cdot \frac{\partial \hat{y}^{&amp;lt;t&amp;gt;}}{\partial z} \cdot 1 = \frac{\partial L_t}{\partial z} = \hat{y}^{&amp;lt;t_l&amp;gt;} -y^{&amp;lt;t_l&amp;gt;} \)&lt;/li&gt;
  &lt;li&gt;\( \frac{\partial a^{&amp;lt;t&amp;gt;}}{\partial W_{aa}} = (1-a^{2&amp;lt;t&amp;gt;})a^{&amp;lt;t-1&amp;gt;} \)&lt;/li&gt;
  &lt;li&gt;\( \frac{\partial a^{&amp;lt;t&amp;gt;}}{\partial b_a} = (1-a^{2&amp;lt;t&amp;gt;}) \)&lt;/li&gt;
  &lt;li&gt;\( \frac{\partial a^{&amp;lt;t&amp;gt;}}{\partial W{ax}}  = (1-a^{2&amp;lt;t&amp;gt;}) \cdot x^{&amp;lt;t&amp;gt;} \)&lt;/li&gt;
  &lt;li&gt;\( \frac{\partial L_t}{\partial W_{ya} } = (\hat{y}^{&amp;lt;t_l&amp;gt;} -y^{&amp;lt;t_l&amp;gt;}) W_{ya} \)&lt;/li&gt;
  &lt;li&gt;\( \frac{\partial a^{&amp;lt;t&amp;gt;}}{\partial a_{&amp;lt;t-1&amp;gt;}} = (1-a^{2&amp;lt;t&amp;gt;})W_{aa} \)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;backpropagation-through-a-network&quot;&gt;Backpropagation through a network&lt;/h3&gt;
&lt;p&gt;After we’ve implemented our backward pass through the network for a single cell. Now we just have to repeat this for our network. The visualization below helps you to understand it better:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/_images/bacterial_names/loss_compute.png&quot; alt=&quot;RNN network - computing loss and backpropagation&quot; /&gt;
&lt;a href=&quot;https://www.coursera.org/learn/intro-to-deep-learning&quot;&gt;Image Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This backpropagation is called a backpropagation &lt;em&gt;‘through time’&lt;/em&gt;. If you think about it, we have to go in the opposite, i.e. reversed order to compute the desired gradient. We need to perform an iterative stepping in reversed order over each time step while &lt;strong&gt;incrementing&lt;/strong&gt; (adding) the overall gradients: \( \partial b_a, \partial W_{aa}, \partial W_{ax} \).&lt;/p&gt;

&lt;h4 id=&quot;gradient-clipping&quot;&gt;Gradient clipping&lt;/h4&gt;

&lt;p&gt;After backpropagation and computing all the gradients we can make sure our gradients would not “explode”, i.e. reach very high values compared to inputs. This phenomenon could make it very hard to effectively train our model. Here is some nice visualization of Loss function descending (gradient descent) with and without clipping:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/_images/bacterial_names/clipping.png&quot; alt=&quot;Gradient clipping&quot; /&gt;
&lt;a href=&quot;https://www.coursera.org/learn/intro-to-deep-learning&quot;&gt;Image Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Gradient clipping keeps the gradients values &lt;em&gt;‘in check’&lt;/em&gt;, that is between some arbitrary &lt;code class=&quot;highlighter-rouge&quot;&gt;min&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;max&lt;/code&gt; values.&lt;/p&gt;

&lt;p&gt;Training the model is performed in a standard fashion, compute loss, calculate gradient, update parameters, and iterate over # number of epochs through the data, trying to minimize this loss function. The key idea is then to get to know how to generate new observations…&lt;/p&gt;

&lt;h2 id=&quot;sampling-the-model&quot;&gt;Sampling the model&lt;/h2&gt;
&lt;p&gt;Sampling is the process of generating novel &lt;em&gt;observations&lt;/em&gt;. In our example new letters would construct a made-up bacterial genera name.&lt;/p&gt;

&lt;p&gt;If our network is trained we can then pass a vector of zeros \( \vec{0}\) as input hidden state \( a^{&amp;lt;0&amp;gt;} \). Then we perform propagation through the first unit of our RNN network. So we obtain next hidden state \(a^{&amp;lt;t+1&amp;gt;}\) and prediction \( \hat{y}^{&amp;lt;t+1&amp;gt;} \) that represents the probabilities of each character &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;\[ a^{&amp;lt;t+1&amp;gt;} = tanh(W_{ax}x^{&amp;lt;t&amp;gt;} + W_{aa}a^{&amp;lt;t&amp;gt;} + b) \]
\[ \hat{y}^{&amp;lt;t+1&amp;gt;}  = softmax( W_{ya}a^{&amp;lt;t+1&amp;gt;} + b_y  )  \]&lt;/p&gt;

&lt;p&gt;Having computed a vector of probabilities \( \hat{y}^{&amp;lt;t+1&amp;gt;}\) we now perform &lt;strong&gt;sampling&lt;/strong&gt; procedure. We do not pick just the highest probability, this would in turn generate the same results each time for a given dataset. We do not want to pick our characters randomly, as results would become random, and all the architecture build would become useless. The key is to select (i.e. &lt;em&gt;sample&lt;/em&gt;) from our \( \hat{y}^{&amp;lt;t+1&amp;gt;} \) distribution.&lt;/p&gt;

&lt;p&gt;In other words, we’ll pick the index &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt; (remember that we have a one-hot encoded our alphabeth) with the probability encoded by the &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt;-th index in \(\hat{y}^{&amp;lt;t+1&amp;gt;} \) matrix.&lt;/p&gt;

&lt;p&gt;The final step is to &lt;strong&gt;overwrite&lt;/strong&gt; the variable \( x^{&amp;lt;t+1&amp;gt;}\) with our predicted one-hot encoding \(y^{&amp;lt;t&amp;gt;}\) of selected/sampled index &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt; from the previous step. This is represented as red arrow on the picture below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/_images/bacterial_names/sampling2.png&quot; alt=&quot;Gradient clipping&quot; /&gt;
&lt;a href=&quot;https://www.coursera.org/learn/intro-to-deep-learning&quot;&gt;Image Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’ll continue this propagation until we reach end of the line character &lt;code class=&quot;highlighter-rouge&quot;&gt;\n&lt;/code&gt;. Then our generation is finished, and we can print our result. If something goes wrong, then we additionally limit ourselves to an arbitrary number of character limit, for example &lt;code class=&quot;highlighter-rouge&quot;&gt;50&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;building-a-language-models&quot;&gt;Building a language models&lt;/h2&gt;

&lt;p&gt;The number of our network parameters is not dependent on the length of the input word. For training purposes we’ll then just loop over one example at a time. Meaning, we’ll forward propagate through RNN architecture, compute and clip gradients, update initial parameters with these computed gradients. Updating parameters means subtracting computed gradients that were multiplied by so called &lt;em&gt;learning rate&lt;/em&gt;, an arbitrary and small value, like. &lt;code class=&quot;highlighter-rouge&quot;&gt;.0001&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;
&lt;p&gt;Using &lt;strong&gt;7901&lt;/strong&gt; genera names from SILVA database I’ve trained this network and came up with a bunch of original bacterial names. Original in this context means that these names have not been present in our input database.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;- Nitronella
- Sebacter
- Vetia
- Setinelfonax
- Vestaphylococcus
- Setonas
- Nembacterium
- Pioclococclus
- Detiptonus
- Frreptococcus
- Teeutomonas
- Fetiphylococcus
- Blunna
- Alococella
- Tantatum
- Cublia
- Palibacter
- Arstrosa
- Glymia
- Actoboctellibacterium
- Salanillus
- Sardaera&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;My presonal favourite is &lt;code class=&quot;highlighter-rouge&quot;&gt;Nitronella&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Vetia&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Frreptococcus&lt;/code&gt;. You can inspect the &lt;code class=&quot;highlighter-rouge&quot;&gt;genera.txt&lt;/code&gt; that I’ve obtained from the training dataset - the name is not there. Instead you can find:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Nitrobacter, Nitrococcus, Nitrolancetus, Nitrosococcus, Nitrosomonas, Nitrosopelagicus, Nitrosospira, Nitrospina, Nitrospinae, Nitrospira, Nitrospirae, Nitrospirillum&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As for bacteria ending with &lt;code class=&quot;highlighter-rouge&quot;&gt;*ella&lt;/code&gt;, there are 425 such observations, some of which are: &lt;code class=&quot;highlighter-rouge&quot;&gt;Volvariella, Nidorella, Veillonella, Weissella, Actinanthella, Truncocolumella, Traorella, Trichinella, Raoultella, Gloeotulasnella&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Are these names useful? Perhaps. I think that names should be descriptive of origin or functionality, and it is really not my field of expertise to have a strong opinion here. But at the same time I feel that it might be a better alternative than naming a bacteria after a researcher.&lt;/p&gt;

&lt;p&gt;Perhaps the next newly discovered bacteria will take its name from similar list? All things considered we’re in the middle of AI revolution, and letting AI to name a new species will inevitably be regarded as a mark in OUR history.&lt;/p&gt;

&lt;h1 id=&quot;resources&quot;&gt;Resources&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.deeplearningbook.org/&quot;&gt;Deep Learning Book by Aaron Courville, Ian Goodfellow, and Yoshua Bengio&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.manning.com/books/deep-learning-with-python&quot;&gt; Deep Learning with Python book by François Chollet &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.coursera.org/learn/intro-to-deep-learning&quot;&gt; National Research University Higher School of Economics: “Introduction to Deep Learning”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.coursera.org/learn/nlp-sequence-models/home/welcome&quot;&gt;Dr Andrew Ng’s “Sequence Models” course&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 21 May 2018 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2018/05/21/RNN-bacteria-name-generator/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/21/RNN-bacteria-name-generator/</guid>
        
        
        <category>Data-Science</category>
        
      </item>
    
      <item>
        <title>Simple anomaly detection system</title>
        <description>&lt;p&gt;In this post I’ll quickly go over a simple &lt;em&gt;anomaly detection system&lt;/em&gt;: what is it, what are its motivations and how to build it. I’ll use an example dataset of hypothyroid patients to look for anomalies. You can read this post, &lt;a href=&quot;https://github.com/vaxherra/vaxherra.github.io/blob/master/_files/anomaly_detection/anomaly.ipynb&quot;&gt;&lt;b&gt;&lt;u&gt;follow along a jupyter notebook &lt;/u&gt;&lt;/b&gt;&lt;/a&gt; for code snippets or &lt;a href=&quot;/_files/anomaly_detection/anomaly.ipynb&quot;&gt;&lt;b&gt;&lt;u&gt;download&lt;/u&gt;&lt;/b&gt;&lt;/a&gt; it for a local use. I obtained the dataset from &lt;a href=&quot;http://odds.cs.stonybrook.edu/thyroid-disease-dataset/&quot;&gt;&lt;em&gt;Outlier Detection Dataset&lt;/em&gt; website&lt;/a&gt;, but you can download it &lt;a href=&quot;/_files/anomaly_detection/thyroid.mat&quot;&gt;&lt;b&gt;&lt;u&gt;here&lt;/u&gt;&lt;/b&gt;&lt;/a&gt; if something happens to the source.&lt;/p&gt;

&lt;font size=&quot;1&quot;&gt;Cover photo source: a cropped fragment from Pieter's Bruegel the Elder &lt;i&gt;'Netherlandish Proverbs'&lt;/i&gt;. This painting is a list of proverbs and idioms, and this particular fragments means &quot;To be able to tie even the devil to a pillow&quot;, i.e. Obstinacy overcomes everything &lt;a href=&quot;https://en.wikipedia.org/wiki/Netherlandish_Proverbs&quot;&gt;[ref].&lt;/a&gt;&lt;/font&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Anomaly detection&lt;/em&gt; is really an &lt;em&gt;outlier&lt;/em&gt; detection problem. Given a certain set of observations we want to build some kind of understanding (model), so given a new example we can determine if it matches our previous observations or is not coming from the same distribution. An anomaly is different from standard classification problem due to the nature of our dataset. Often in anomaly detection we are given a dataset with &lt;em&gt;skewed classes&lt;/em&gt;, i.e. we have much more &lt;em&gt;negative&lt;/em&gt; (non-anomalous) that positive (anomalous) example. You can imagine a highly efficient production system, where most of our products are of good, acceptable quality, and are ready to be sold. Sometimes, however, we report a faulty product that cannot go into the market. Imagine that out of 100 thousand examples only 125 were &lt;em&gt;positive&lt;/em&gt; (anomalous). So only 0.125% of examples are &lt;em&gt;positive&lt;/em&gt;. Which is good for our production system, but makes it harder to use state of the art, ‘data-hungry’ machine learning algorithms based on neural nets (NN). NNs would probably not be able to properly learn what an &lt;em&gt;anomaly&lt;/em&gt; means. We often say, there is an infinite number of ways something might go wrong, but usually, there is a limited number of ways to do something properly.&lt;/p&gt;

&lt;p&gt;Let’s say that we can automatically or semi-automatically collect some features from our products. Given the sheer number of features, it is often not possible to manually make sure our system or product it’s working properly. But wait. Didn’t I say that our example is going to include hypothyroid patients? Yes, indeed. For biological problems, this might be even harder, as biological systems usually are characterized by huge variability.&lt;/p&gt;

&lt;h3 id=&quot;hypothyroid-dataset-overview&quot;&gt;Hypothyroid dataset overview&lt;/h3&gt;
&lt;p&gt;I’ve already mentioned that our dataset is related to the hypothyroid disease. I’ve obtained this dataset from &lt;a href=&quot;http://odds.cs.stonybrook.edu/thyroid-disease-dataset/&quot;&gt;Outlier Detection Datasets&lt;/a&gt; website. It comprises of 3772 subjects, only 93 subjects are characterized as hypothyroid (i.e. positive, anomalous) ( \( 2.5\% \) ). Each subject is attributed with six real-value features that we must use to build a model and predict whether it is or might be hypothyroid patient or not. In addition data has \( \hat{Y} \in {0,1 } \) labels, that state the &lt;em&gt;‘ground truth’&lt;/em&gt;, so we know whether a certain subject was actually hypothyroid or not. In fact, if you read the data described in the provided link, you can see that dataset merges two classes (normal and subnormal functioning) into one “normal class”. Unfortunately, features are not named, so we actually don’t know what each of them represents. But for learning purposes, this is enough. Ok, let’s proceed to formulate a problem for our dataset.&lt;/p&gt;

&lt;h3 id=&quot;problem-formulation&quot;&gt;Problem formulation&lt;/h3&gt;
&lt;p&gt;Suppose we are working as a (clinical) data analyst for some medical organization close to a GP. One day we are given a moderate in size dataset of our patients. The staff has been collecting some six quantifiable features, be it symptoms or tests, that are indicative (to a limited extent) or suggest a hypothyroidism - “a condition in which the thyroid gland doesn’t produce enough thyroid hormone”. Our data also has a label stating whether a patient actually was hypothyroid.&lt;/p&gt;

&lt;p&gt;We are not running a thyroid diagnosis and treatment center, but are a part of first contact team. For doctors, it would be nice to build some model that detects anomalies in patients, so further efforts might focus on specific organs or targeted diagnostics. Say, patient visits a GP, goes over symptoms and has a set of basic tests and measurements. Given our data, we can’t really tell whether this patient is or is not hypothyroid, but rather whether there is some significant abnormality in a set of his results that, given small historical data, we may want to send him or her to check specifically for thyroid.&lt;/p&gt;

&lt;p&gt;Our dataset is relatively small and skewed as most patients do not have hypothyroidism. This limits us directly in using “state of the art”, everyone’s favorite neural net classification system. However, we can try working with this data and construct an anomaly detection system.&lt;/p&gt;

&lt;h2 id=&quot;building-and-testing-a-model&quot;&gt;Building and testing a model&lt;/h2&gt;

&lt;p&gt;The idea is simple. We model each feature of our dataset by a multivariate Gaussian distribution: compute mean matrix ( \( \mu \)  ) and covariance matrix ( \( \Sigma \) ) on a training set:&lt;/p&gt;

&lt;p&gt;\[ \mu = \frac{1}{m} \Sigma^m_i x^{(i)} \]&lt;/p&gt;

&lt;p&gt;\[ \Sigma = \frac{1}{m} \Sigma^m_i (x^{(i)}-\mu)\cdot(x^{(i)}-\mu)^T \]&lt;/p&gt;

&lt;p&gt;Mean matrix \( \mu \) contains a mean value for each feature &lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt;. Be sure to distinguish a summation sigma \( \Sigma_i^m \) from a covariance matrix sigma \( \Sigma \). This can be misleading, but just a bit. Then, given a new example \( x^{i}_1,…,x^{i}_n \) (with &lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt; features) we can compute its probability as defined by Multivariate Gaussian Distribution:&lt;/p&gt;

&lt;p&gt;\[ f_x(x^{i}) = \frac{  exp(- \frac{1}{2} (x^i-\mu)^T  \Sigma^{-1} (x^i-\mu)  ) }{  \sqrt{2\pi^n  | \Sigma |    }} \]&lt;/p&gt;

&lt;p&gt;Having computed probability of a given, new example, we can then decide based on that single number whether it is coming from our distribution or not. However, we should also determine a probability threshold \( \epsilon \) below which we consider an example as &lt;em&gt;anomalous&lt;/em&gt;. But before we do that, we have to intelligently split our dataset.&lt;/p&gt;

&lt;p&gt;We need to reasonably split the dataset for training, cross-validation and model testing. The idea is that for model training we use only negative examples, i.e. non-anomalous. Since we want to model each feature with a Gaussian distribution, it would be appropriate to &lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt; our model on negative examples and use a smaller portion of positive examples for hyperparameter tuning (\( \epsilon \) ) and model testing.&lt;/p&gt;

&lt;p&gt;Table below shows how one might approach it for our hypothyroid dataset:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Sets&lt;/th&gt;
      &lt;th&gt;# Negative examples&lt;/th&gt;
      &lt;th&gt;# Positive examples&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Train set&lt;/td&gt;
      &lt;td&gt;2999&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cross-validation set&lt;/td&gt;
      &lt;td&gt;340&lt;/td&gt;
      &lt;td&gt;46&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Test set&lt;/td&gt;
      &lt;td&gt;340&lt;/td&gt;
      &lt;td&gt;47&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So train set is used to compute mean matrix \( \mu \) and covariance matrix \( \Sigma \). Cross-validation set is used to set an \( \epsilon \) probability threshold. How do we do this? We loop over an arbitrary number of possible probabilities, say one thousand or million points between the minimum and maximum probability obtained from a cross-validation set. Since our classes (anomalous vs non-anomalous) are skewed (there is disproportionately more negative examples that positives) we cannot use simple &lt;em&gt;accuracy&lt;/em&gt; based on frequency, i.e. how many times our classifier was correct. Imagine that we predict \( y=0 \) all the time. Since the majority of our subjects are non-anomalous this would actually classify wrongly  93 out of 3772 and that would give us 97.53% accuracy. But this is just wrong. We do want to catch some anomalies if they likely occur, even if we might be wrong about them, as it is essentially better to send a non-hypothyroid patient for additional screening than to neglect actual hypothyroidism in a patient. We’d be happier even if we traded some of the precision for a recall.&lt;/p&gt;

&lt;p&gt;Thus, we must operate using &lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;F1&lt;/code&gt; score&lt;/a&gt; based on measure of precision and recall calculated using true positives (&lt;code class=&quot;highlighter-rouge&quot;&gt;tp&lt;/code&gt;), false positives (&lt;code class=&quot;highlighter-rouge&quot;&gt;fp&lt;/code&gt;) and false negatives (&lt;code class=&quot;highlighter-rouge&quot;&gt;fn&lt;/code&gt;):&lt;/p&gt;

&lt;p&gt;\[ F_1 = \frac{2 \cdot precision \cdot recall}{precision + recall} \]&lt;/p&gt;

&lt;p&gt;\[ precision =  \frac{tp}{tp+fp} \]&lt;/p&gt;

&lt;p&gt;\[ recall = \frac{tp}{tp+fn}  \]&lt;/p&gt;

&lt;p&gt;Just a reminder. True positive refer to anomalous examples. If, for a given \( \epsilon \) over which we are iterating the computed probability \( f_x^i &amp;lt; \epsilon \) and our ground truth label \( y=1 \) then we can count an example as true positive. False negative would occur for the same true label, but we’d observe \( f_x^i &amp;lt;&amp;gt; \epsilon \). And finally a false positive would produce a small probability, below-given threshold \( f_x^i &amp;lt; \epsilon \), but in reality, the truth label says it’s a non-anomalous example \( y=0 \).&lt;/p&gt;

&lt;p&gt;Iterating over a set of \( epsilon \) values we choose the one that minimizes errors, i.e. maximizes our F1 score. We then use this \( \epsilon \) on the third portion of our data - &lt;em&gt;test set&lt;/em&gt; and compute final precision, recall, and F1 scores.&lt;/p&gt;

&lt;p&gt;If you follow &lt;a href=&quot;https://github.com/vaxherra/vaxherra.github.io/blob/master/_files/anomaly_detection/anomaly.ipynb&quot;&gt;&lt;b&gt;&lt;u&gt;jupyter notebook&lt;/u&gt;&lt;/b&gt;&lt;/a&gt; for this post, then you observed that our final F1 score is \( F1 \approx 0.73 \) with \( precision \approx 0.58 \) and  \( recall \approx 0.96 \). Is this good?&lt;/p&gt;

&lt;h2 id=&quot;final-comment&quot;&gt;Final comment&lt;/h2&gt;

&lt;p&gt;Our model has a high recall, which means we very well identify all anomalies. So we are “good” at catching anomalies in patients when they occur, however when you look at our precision score, it is low. Precision essentially measures how well (precisely) our model identifies anomalies. I.e. our low score indicates that model will produce an “alert” even if some patient might not be hypothyroid. Is this good? Given the skewed dataset, and serious lack of positive examples I’d say it is helpful. When a new patients comes in, we perform a given set of basic tests, and a model predicts that he or she might be hypothyroid, we just sent this patient for a detailed set of tests that are more precise (or sensitive). This is not a serious decision on performing a complicated operation relying on our system. But at the same time, we make sure that we are catching \( \approx 96\% \) of anomalies, as our recall score indicates. Imagine you have a huge turnouver of patients in a first-contact clinic. Spending less time on manually looking at results and wondering whether they might be indicative of malfunctioning thyroid saves time for a GP. Also, at certain situations thus might &lt;em&gt;at hoc&lt;/em&gt; suggest hypothyroid disease, and drive a GP to ask questions specific to symptoms (feeling increasingly tired, have dry skin, constipation and weight gain) or directly send a patient for a sensitive TSH test.&lt;/p&gt;

&lt;p&gt;Oftentimes building a model and estimating accuracy is not enough. We need a model interpretation for a particular use. We have to analyze the nature of the problem and think about desired output. Here we have a standard precision and recall tradeoff, in which we are far better of with higher recall.&lt;/p&gt;
</description>
        <pubDate>Tue, 15 May 2018 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2018/05/15/anomaly-detection/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/15/anomaly-detection/</guid>
        
        
        <category>Data-Science</category>
        
      </item>
    
      <item>
        <title>Homepage Launches</title>
        <description>&lt;p&gt;Welcome to my homepage!&lt;/p&gt;

&lt;p&gt;I am Robert Kwapich. I studied medical physics and obtained a Master’s degree from the University of Silesia in Katowice, Poland. I am currently working in the domain of bioinformatics and data-science as a Research Assistant in the US of A. I enjoy playing with machine learning and deep learning methods for data science in biology and medicine. Read more about me on the &lt;a href=&quot;/&quot;&gt;main page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The intent of this website is double-fold. I want to present some of my ideas, thoughts and inspirations to the world in the form of blog posts. They can range from cinema, paintings, music, philosophy to physics, biology, computer science and some branches of artificial intelligence. The second pillar of my website is to present my actual work and implemented ideas in code. They can serve as my portfolio, or as a reference for somebody interested. Each post has a comment box hosted on &lt;strong&gt;disqus&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;I plan to publish every 6-8 weeks in several categories: blog, data-science and bioinformatics. Right now you can navigate to my first post about &lt;a href=&quot;_posts/2018-05-11-anomaly-detection&quot;&gt;simple anomaly detection&lt;/a&gt; or to the &lt;a href=&quot;/&quot;&gt;main page&lt;/a&gt; and &lt;em&gt;‘about me’&lt;/em&gt; tab.&lt;/p&gt;

&lt;p&gt;Feel free to say hello in the comment!&lt;/p&gt;
</description>
        <pubDate>Sat, 12 May 2018 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2018/05/12/launch/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/12/launch/</guid>
        
        
        <category>Blog</category>
        
      </item>
    
  </channel>
</rss>
